{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+D+j717bT0xnYEX6TCNBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vin136/NN-foundations/blob/main/HW2_sols.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "dWIJp4JTx1td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            self.priors[i] = np.sum(y == self.classes[i]) / n_samples\n",
        "        \n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        log_probs = np.log(self.priors) + X @ np.log(self.counts).T\n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pkH1OvWXx0og"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the multinomial Naive Bayes model, each document is represented as a bag of words and the number of occurrences of each word is used as a feature.\n",
        "\n",
        "Given a set of m training documents, C classes, and a vocabulary of n words, let x be a new document represented as a bag of words, where x_i is the count of the i-th word in the vocabulary in the document. The goal is to find the class y that maximizes the posterior probability, P(y|x), using Bayes' Theorem:\n",
        "\n",
        "$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
        "\n",
        "Here, P(y) is the prior probability of the class, which can be estimated as the fraction of documents in the training set that belong to class y. P(x|y) is the likelihood of the document given the class, which can be estimated as the product of the probabilities of each word in the vocabulary given the class. P(x) is the normalizing constant, which is the same for all classes and can be ignored for the purposes of estimation.\n",
        "\n",
        "Using the multinomial distribution, the likelihood can be written as:\n",
        "\n",
        "$P(x|y) = \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "Where P(x_i|y) is the probability of observing word i in a document given class y, which can be estimated as the count of word i in class y divided by the total count of all words in class y.\n",
        "\n",
        "Finally, taking the log of the posterior probabilities makes the calculation easier and allows us to find the MAP estimate by simply taking the maximum value:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log P(y|x) &= \\log \\frac{P(x|y)P(y)}{P(x)} \\\\\n",
        "&= \\log P(x|y) + \\log P(y) - \\log P(x) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "For prediction's we can ignore $\\log P(x)$ term and report the y that has highest $\\log P(y = i|x)$, where $i = {1,\\dots,c}$."
      ],
      "metadata": {
        "id": "cp5ohemuyLvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "PIthJ5MMx-4k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here fit calculates the p(y) and also stores two sets of parameters p(x|y) for y=0 and y=1. Let's check these before proceeding."
      ],
      "metadata": {
        "id": "1NX4IzXoyjls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb.priors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYakGrx7yiZ2",
        "outputId": "bbb5dce4-f61d-4e3a-fd1b-c2ed0d7473d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in estimating \n",
        "nb.counts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgKXy9J8y5mJ",
        "outputId": "99a0feea-546e-460c-e3ae-fd69f600f4db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MXt-esstzlBQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use this cell to further understand the above code"
      ],
      "metadata": {
        "id": "byWjinRrzynN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Complete the missing line in `predict function` [25 points]"
      ],
      "metadata": {
        "id": "I61FIcvuyjDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            self.priors[i] = np.sum(y == self.classes[i]) / n_samples\n",
        "        \n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        log_probs = np.log(self.priors) + X @ np.log(self.counts).T\n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_5hLOnmB0yDV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Now let's test the effectivness of this algorithm on a real-world data set. Run the below cells and report the accuracy on the test set ? [20 POINTS]"
      ],
      "metadata": {
        "id": "VDz_PuKexylM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans: "
      ],
      "metadata": {
        "id": "l18kcS9V2NVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Convert the text data into a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n"
      ],
      "metadata": {
        "id": "QR48XYIoteNl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set has 20 different types of news"
      ],
      "metadata": {
        "id": "QEiXOOnk1grq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroups_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH8qCeWV1pw3",
        "outputId": "a7b9dab6-da0a-4d43-f6c9-073b7e1449ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's see a sample\n",
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "rMZHDfH608dF",
        "outputId": "207c1547-c88b-4259-a50e-bdac952ea192"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The target attribute is the integer index of the category\n",
        "newsgroups_train.target[0],list(newsgroups_train.target_names)[newsgroups_train.target[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KkXB0j1KSD",
        "outputId": "7e80cf6b-1349-4505-ed32-45e1dcdbf208"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 'rec.autos')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the multinomial Naive Bayes model on the training data\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Predict the class labels of the testing samples\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = np.mean(y_pred == newsgroups_test.target)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiAMATGcuQOY",
        "outputId": "8d47f6ac-1ce6-4fdb-c69c-89b114fd9376"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4626925119490175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. In your code for calculating the counts(`self.counts`) \n",
        "\n",
        "```\n",
        "self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "```\n",
        "\n",
        "```\n",
        " self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        " ```\n",
        "\n",
        " Why do we add 1 in the numerator and include n_features ? Give a short explanation ? [25 points]\n",
        "\n",
        " Hint: It's called Laplace smoothing. Figure out why it's being used here."
      ],
      "metadata": {
        "id": "tVtPN5h72WR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL:\n",
        "\n",
        "To deal with unseen words. It acts as a prior. (student can explain this further)"
      ],
      "metadata": {
        "id": "kv_Fgswz_3UC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qAivNSm5Jdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-m5tItVhZfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this data set\n",
        "\n",
        "| Feature 1 | Feature 2 | Class |\n",
        "|-----------|-----------|-------|\n",
        "| 1         | 1         | 0     |\n",
        "| 2.2         | 1.6         | 0     |\n",
        "| 2.5         | 1.8         | 0     |\n",
        "| 2.8         | 1.5         | 0     |\n",
        "| 2.9         | 1.2         | 0     |\n",
        "| 3.0        | 3.0        | 1     |\n"
      ],
      "metadata": {
        "id": "LuR9P1Wk6PgX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaGJXsgR8lYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Below code plots the decision boundary of the logistic regression on this dataset. Look at the decision boundary and answer the following questions : [30 points]\n",
        "\n",
        "a. What's the accuracy of logistic regression on this dataset ?[10 POINTS]\n",
        "\n",
        "b. Why do you think it misclassified class 1 ? (explain in terms of the loss function) [10 POINTS]\n",
        "\n",
        "c. This is an example of class-imbalanced data set(class frequency is heavily skewed). As we noted below, Logistic regression might give unintuitive decision boundary. Suggest any practical way of dealing with this problem ? [10 POINTS]\n",
        "\n",
        "(you are free to search online, but understand the proposed method and write your response clearly explaining how it would potentially solve this problem)"
      ],
      "metadata": {
        "id": "cUNuAJpE8lxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol:\n",
        "\n",
        "a. 5/6\n",
        "\n",
        "b. The cost function can be mimized even after correctly classifying a point by pushing the decision boundary away from the points. Wnen one of the class has high frequency this can come at misclassifying the minority class. (student can further prove this claim by writing down the loss function)\n",
        "\n",
        "c. Use class priors, use a mixture model etc. Or even use a different type of classifier. (SVM etc). (give credit as long as student explains his approach that logically makes sense.)"
      ],
      "metadata": {
        "id": "5T_9mk_U_I8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the dataset\n",
        "X = np.array([[1, 1.0], [2.2, 1.6], [2.5, 1.8], [2.8, 1.5], [2.9, 1.2], [3.0, 3.0]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1])\n",
        "\n",
        "# Fit logistic regression and SVM models\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "\n",
        "\n",
        "# Define a grid of points to visualize the decision boundary\n",
        "xx, yy = np.meshgrid(np.arange(0, 10, 0.1),\n",
        "                     np.arange(0, 10, 0.1))\n",
        "Z_lr = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z_lr = Z_lr.reshape(xx.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Plot the decision boundary and the dataset\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.contour(xx, yy, Z_lr, colors='red', alpha=0.5)\n",
        "#ax.contour(xx, yy, Z_svm, colors='blue', alpha=0.5, levels=[0])\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "ISFpA-Qf5KTp",
        "outputId": "a81c78b3-2451-4617-a981-e788df7ef34b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eaZtcp8ExxjE"
      }
    }
  ]
}