{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "dWIJp4JTx1td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pkH1OvWXx0og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the multinomial Naive Bayes model, each document is represented as a bag of words and the number of occurrences of each word is used as a feature.\n",
        "\n",
        "Given a set of m training documents, C classes, and a vocabulary of n words, let x be a new document represented as a bag of words, where $x_i$ is the count of the i-th word in the vocabulary in the document. The goal is to find the class y that maximizes the posterior probability, $P(y|x)$, using Bayes' Theorem:\n",
        "\n",
        "$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
        "\n",
        "Here, $P(y)$ is the prior probability of the class, which can be estimated as the fraction of documents in the training set that belong to class y. $P(x|y)$ is the likelihood of the document given the class, which can be estimated as the product of the probabilities of each word in the vocabulary given the class. $P(x)$ is the normalizing constant, which is the same for all classes and can be ignored for the purposes of estimation.\n",
        "\n",
        "Using the multinomial distribution, the likelihood can be written as:\n",
        "\n",
        "$P(x|y) = \\prod_{i=1}^n P(x_i|y)$\n",
        "\n",
        "Where $P(x_i|y)$ is the probability of observing word i in a document given class y, which can be estimated as the count of word i in class y divided by the total count of all words in class y.\n",
        "\n",
        "Finally, taking the log of the posterior probabilities makes the calculation easier and allows us to find the MAP estimate by simply taking the maximum value:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log P(y|x) &= \\log \\frac{P(x|y)P(y)}{P(x)} \\\\\n",
        "&= \\log P(x|y) + \\log P(y) - \\log P(x) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "For prediction's we can ignore $\\log P(x)$ term and report the y that has highest $\\log P(y = i|x)$, where $i = {1,\\dots,c}$."
      ],
      "metadata": {
        "id": "cp5ohemuyLvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above implementation has two missing parts. Let's develop it step by step."
      ],
      "metadata": {
        "id": "skTv0AJO5HwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: PART A [5 points]\n",
        "\n",
        "Before writing any code, consider the following toy input. After fitting the dataset, what should be the value in `self.priors` ?\n",
        "\n",
        "Note: you need to just write down the answer."
      ],
      "metadata": {
        "id": "kS2NM7s_5l19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol:"
      ],
      "metadata": {
        "id": "R668ansf6SOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PIthJ5MMx-4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1: PART B [10 points]\n",
        "\n",
        "In the `fit` function fill the missing line by uncommenting `self.priors[i] = ` and also remove `pass` after doing so. \n",
        "\n",
        "Note: Make edits in the below template.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vG_d4FoO6UUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "        # Calculate the log probability of each class for each sample\n",
        "        #log_probs = \n",
        "        \n",
        "        # Return the class with the highest log probability for each sample\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n"
      ],
      "metadata": {
        "id": "g0z_u9VG6hoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n"
      ],
      "metadata": {
        "id": "FsdnL31MD7PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here fit calculates the p(y) and also stores two sets of parameters p(x|y) for y=0 and y=1. Let's check these before proceeding. (run the below cells)"
      ],
      "metadata": {
        "id": "1NX4IzXoyjls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb.priors"
      ],
      "metadata": {
        "id": "lYakGrx7yiZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nb.counts.shape"
      ],
      "metadata": {
        "id": "zgKXy9J8y5mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb.counts"
      ],
      "metadata": {
        "id": "MXt-esstzlBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use this cell to further understand the above code"
      ],
      "metadata": {
        "id": "byWjinRrzynN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: PART C [10 points]\n",
        "\n",
        "Complete the `log_probs = ` line in predict function. (code block is pasted below)\n",
        "\n",
        "Note: If you can't implement in a single line you are free to write it in multiple lines. If your implementation is correct below `assert` statement should run with no error."
      ],
      "metadata": {
        "id": "I61FIcvuyjDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        # Calculate the number of classes and store the class labels\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        \n",
        "        # Calculate the number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Calculate the class priors\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for i in range(n_classes):\n",
        "            #self.priors[i] = \n",
        "            pass\n",
        "        # Calculate the class-conditional feature probabilities\n",
        "        self.counts = np.zeros((n_classes, n_features))\n",
        "        for i in range(n_classes):\n",
        "            X_class = X[y == self.classes[i],:]\n",
        "            self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "        self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        "        \n",
        "    def predict(self, X):\n",
        "      # returns a list of class labels for each x in X.\n",
        "      # Calculate the log probability of each class for each sample\n",
        "      #log_probs = \n",
        "        \n",
        "      # Return the class with the highest log probability for each sample\n",
        "      return self.classes[np.argmax(log_probs, axis=1)]\n"
      ],
      "metadata": {
        "id": "_5hLOnmB0yDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnqdDx34EctS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we assume n = 4, and each X, say [1,1,2,0] represents the corresponding counts of each of those words in that sentence.\n",
        "X = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 2, 1, 2], [1, 1, 0, 2]])\n",
        "# we have two classes \n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(X, y)\n",
        "\n",
        "assert np.all(nb.predict(X)==y),\"your predictions are wrong\""
      ],
      "metadata": {
        "id": "00SpKDb88Nei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the effectivness of this algorithm on a real-world data set. "
      ],
      "metadata": {
        "id": "VDz_PuKexylM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#look at the below cells and understand the dataset."
      ],
      "metadata": {
        "id": "QR48XYIoteNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZwDRjr_9gBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data set has 20 different types of news"
      ],
      "metadata": {
        "id": "QEiXOOnk1grq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GJc0b0vf-Cz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(newsgroups_train.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKf5h-6n-Paj",
        "outputId": "dcd572c2-f7fa-4d7c-ae64-db242cc890a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroups_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH8qCeWV1pw3",
        "outputId": "a7b9dab6-da0a-4d43-f6c9-073b7e1449ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's see a sample\n",
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "rMZHDfH608dF",
        "outputId": "207c1547-c88b-4259-a50e-bdac952ea192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The target attribute is the integer index of the category\n",
        "newsgroups_train.target[0],list(newsgroups_train.target_names)[newsgroups_train.target[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KkXB0j1KSD",
        "outputId": "7e80cf6b-1349-4505-ed32-45e1dcdbf208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 'rec.autos')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 PART A[10 POINTS]\n",
        "\n",
        "Before proceeding, we need to convert the text into Bag of words format(as we saw in toy example). Sklearn has a built in function for it. Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for it below and fill the missing lines"
      ],
      "metadata": {
        "id": "e3Kb4O9z9vGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text data into a bag-of-words representation\n",
        "\n",
        "vectorizer = \n",
        "X_train = \n",
        "X_test = \n",
        "\n",
        "# Note both train and test data are newsgroups_train.data,newsgroups_test.data respectively. "
      ],
      "metadata": {
        "id": "aUAbRv3P-miY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0407Sew9v-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 PART B[10 POINTS]\n",
        "\n",
        "If your implementation of all the above parts is correct, you should be able to run the below cell. Run the below cell and report the accuracy."
      ],
      "metadata": {
        "id": "eU8MOp3cAW1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:"
      ],
      "metadata": {
        "id": "yYKp9Oz2AnzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the multinomial Naive Bayes model on the training data\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Predict the class labels of the testing samples\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = np.mean(y_pred == newsgroups_test.target)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "HiAMATGcuQOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 PART C [5 Points]\n",
        "\n",
        "In your code for calculating the counts(`self.counts`) \n",
        "\n",
        "```\n",
        "self.counts[i,:] = np.sum(X_class, axis=0) + 1\n",
        "```\n",
        "\n",
        "```\n",
        " self.counts /= np.sum(self.counts, axis=1).reshape(-1, 1) + n_features\n",
        " ```\n",
        "\n",
        " Why do we add 1 in the numerator and include n_features ? Give a short explanation ?\n",
        "\n",
        " Hint: It's called Laplace smoothing. Figure out why it's being used here."
      ],
      "metadata": {
        "id": "tVtPN5h72WR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL:\n",
        "\n"
      ],
      "metadata": {
        "id": "kv_Fgswz_3UC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qAivNSm5Jdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-m5tItVhZfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this data set\n",
        "\n",
        "| Feature 1 | Feature 2 | Class |\n",
        "|-----------|-----------|-------|\n",
        "| 1         | 1         | 0     |\n",
        "| 2.2         | 1.6         | 0     |\n",
        "| 2.5         | 1.8         | 0     |\n",
        "| 2.8         | 1.5         | 0     |\n",
        "| 2.9         | 1.2         | 0     |\n",
        "| 3.0        | 3.0        | 1     |\n"
      ],
      "metadata": {
        "id": "LuR9P1Wk6PgX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaGJXsgR8lYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Below code plots the decision boundary of the logistic regression on this dataset. Look at the decision boundary and answer the following questions : [50 points]\n",
        "\n",
        "\n",
        "* a.What is the accuracy of logistic regression on this dataset ?[10 POINTS]\n",
        "\n",
        "* b. Below plot also shows the decision boundary. In this case can we have a decision boundary that perfectly classifies all the points [10 POINTS]\n",
        "\n",
        "\n",
        "* c. Logistic regression only gives class probabilities, you still need a decision rule to classify. Let the rule be `>= 0.5` for class 1 else class 0. [10 POINTS]\n",
        "\n",
        "\n",
        "\n",
        ">> Now, answer the below statement as TRUE/FALSE and give short explanation\n",
        "\n",
        ">> **Logistic regression assigns zero loss to the points that are correctly classified()**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* d. If we add a new point p = [80.0,80.0] with true label as `1` and remove the point `[3.0,3.0],1` retrain it on the dataset, will p be correctly correctly classified now ? Add a short explanation for your answer.[10 POINTS]\n",
        "\n",
        "* e.In a realworld situation, you are suspecting that your data might have some outliers. For a data-point `x` and the learned weight vector `w` and bias `b`, how will you calculate it's distance from the decision boundary ? Now you might be able to use it to identify outliers. [10 points]\n",
        "\n",
        "Note: This is an example of class-imbalanced data set(class frequency is heavily skewed). As we noted below, Logistic regression might give unintuitive decision boundary. Most machine learning algorithms have trouble dealing with such cases. "
      ],
      "metadata": {
        "id": "cUNuAJpE8lxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sol:\n",
        "\n"
      ],
      "metadata": {
        "id": "5T_9mk_U_I8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the dataset\n",
        "X = np.array([[1, 1.0], [2.2, 1.6], [2.5, 1.8], [2.8, 1.5], [2.9, 1.2], [3.0, 3.0]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1])\n",
        "\n",
        "# Fit logistic regression \n",
        "lr = LogisticRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "\n",
        "# Define a grid of points to visualize the decision boundary\n",
        "xx, yy = np.meshgrid(np.arange(0, 10, 0.1),\n",
        "                     np.arange(0, 10, 0.1))\n",
        "Z_lr = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z_lr = Z_lr.reshape(xx.shape)\n",
        "\n",
        "\n",
        "# Plot the decision boundary and the dataset\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.contour(xx, yy, Z_lr, colors='red', alpha=0.5)\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='black')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "ISFpA-Qf5KTp",
        "outputId": "a81c78b3-2451-4617-a981-e788df7ef34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eaZtcp8ExxjE"
      }
    }
  ]
}