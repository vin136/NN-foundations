{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vin136/NN-foundations/blob/main/cs461_hw3_solutions_draft2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE: Search for \"### FILL IN ###\" to find areas of problem that you are expected to work on."
      ],
      "metadata": {
        "id": "SJ6DcaRsASZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART A: LINEAR REGRESSION [60 POINTS]**"
      ],
      "metadata": {
        "id": "AJFn1ts4Suvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "6zBSSOPBSALS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: Model fitting and prediction for Linear Regression [15 POINTS]**\n",
        "\n",
        "The model parameters of Linear regression can be obtained by optimizing the RSS (residual sum of squares) w.r.t. $\\beta$ : \n",
        "\n",
        "![Screenshot 2023-03-04 at 12.27.46 AM.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "This optimization can be achieved by Gradient Ascent or by using the Closed Form solution directly. The Closed Form solution is as follows: \n",
        "\n",
        "![Screenshot 2023-03-04 at 12.29.07 AM.png](data:image/png;base64,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)\n",
        "\n",
        "This implementation of Linear Regression adds a column of 1s to the input data matrix to account for the bias term, then computes the closed-form solution using the above equation, where $\\beta$ is the vector of weights ($\\beta_1 . . . . \\beta_p$) and bias ($\\beta_0$), and y is the vector of output values. The bias term is stored separately as the first element of beta, and the remaining elements correspond to the weights for the input features."
      ],
      "metadata": {
        "id": "ckO0jQBRA-EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # modifying X to use (weights) and (bias) as a single (beta) term in the closed form solution\n",
        "        # add a column of 1s to X for the bias term\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # compute the closed-form solution\n",
        "        self.betas = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "        # extracting bias as beta_0 and remaining betas as the weights\n",
        "        self.bias = self.betas[0]\n",
        "        self.weights = self.betas[1:]\n",
        "    \n",
        "\n",
        "    def predict(self, X):\n",
        "        # add a column of 1s to X for the bias term\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # make predictions using the learned weights and bias\n",
        "        y_predicted = X @ np.concatenate([[self.bias], self.weights])\n",
        "\n",
        "        return y_predicted\n"
      ],
      "metadata": {
        "id": "zwYu8gMt9xvG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the LinearRegression class to fit a linear regression model on the Diabetes dataset, which is a commonly used dataset in machine learning for regression tasks. \n",
        "\n",
        "The dataset contains 442 samples (N=442), each with 10 input features (p=10) and an output variable."
      ],
      "metadata": {
        "id": "Ho-60eWYEaw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = datasets.load_diabetes()"
      ],
      "metadata": {
        "id": "At2A-tscJQBP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diabetes.feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KskGfGwGMJ3j",
        "outputId": "10a45bcb-ac33-4898-e760-b860a91d6b16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(diabetes.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946LTaxrJT9d",
        "outputId": "0615ac71-da05-43a0-bd4c-ac9badfdc593"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _diabetes_dataset:\n",
            "\n",
            "Diabetes dataset\n",
            "----------------\n",
            "\n",
            "Ten baseline variables, age, sex, body mass index, average blood\n",
            "pressure, and six blood serum measurements were obtained for each of n =\n",
            "442 diabetes patients, as well as the response of interest, a\n",
            "quantitative measure of disease progression one year after baseline.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "  :Number of Instances: 442\n",
            "\n",
            "  :Number of Attributes: First 10 columns are numeric predictive values\n",
            "\n",
            "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
            "\n",
            "  :Attribute Information:\n",
            "      - age     age in years\n",
            "      - sex\n",
            "      - bmi     body mass index\n",
            "      - bp      average blood pressure\n",
            "      - s1      tc, total serum cholesterol\n",
            "      - s2      ldl, low-density lipoproteins\n",
            "      - s3      hdl, high-density lipoproteins\n",
            "      - s4      tch, total cholesterol / HDL\n",
            "      - s5      ltg, possibly log of serum triglycerides level\n",
            "      - s6      glu, blood sugar level\n",
            "\n",
            "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
            "\n",
            "Source URL:\n",
            "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
            "\n",
            "For more information see:\n",
            "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
            "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### looking at first 3 rows of dataset\n",
        "\n",
        "print(diabetes.data[:3])\n",
        "print(\"\\n\")\n",
        "print(diabetes.target[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PlC2dtj5vrk",
        "outputId": "59c11838-0c75-47af-d8b2-2890564ca242"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.03807591  0.05068012  0.06169621  0.02187239 -0.0442235  -0.03482076\n",
            "  -0.04340085 -0.00259226  0.01990749 -0.01764613]\n",
            " [-0.00188202 -0.04464164 -0.05147406 -0.02632753 -0.00844872 -0.01916334\n",
            "   0.07441156 -0.03949338 -0.06833155 -0.09220405]\n",
            " [ 0.08529891  0.05068012  0.04445121 -0.00567042 -0.04559945 -0.03419447\n",
            "  -0.03235593 -0.00259226  0.00286131 -0.02593034]]\n",
            "\n",
            "\n",
            "[151.  75. 141.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the diabetes dataset\n",
        "X, y = diabetes.data, diabetes.target\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bXOuYffJUZM",
        "outputId": "c153006d-2f8f-4aaa-a463-f35bbb59dbab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10)\n",
            "(442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the Diabetes dataset, splits it into train and test sets, creates an instance of the LinearRegression class, fits the model to the training data using the closed-form solution, makes predictions on the test data using the learned weights and bias.\n",
        "\n",
        "**NOTE**: In this exercise we will use our test set as our validation set for finding the optimal value of lambda(as discussed in the class). Assume that the real testing dataset which we use to report the final model performance is unknown to us. In real life machine learning you do not have access to test data.\n",
        "\n",
        "Here we evaluate the performance of the model using MSE (mean squared error), which is a common metric for regression tasks."
      ],
      "metadata": {
        "id": "GxPIHxXkHnp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# create an instance of the LinearRegression class\n",
        "lr = LinearRegression()\n",
        "\n",
        "# fit the model to the training data\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the testing data\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model using mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error (test): {mse}\")\n",
        "\n",
        "# also checking the error on training data\n",
        "y_pred = lr.predict(X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(f\"Mean squared error (train): {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_JtGqOFEaQ3",
        "outputId": "645aa2a7-e6c1-43a6-f144-a5bc4144c87c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error (test): 2900.1936284934795\n",
            "Mean squared error (train): 2868.5497028355776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"### FILL IN ###\" [5 POINTS]\n",
        "\n",
        "Report the training and testing errors.\n",
        "\n",
        "Answer: (test): 2900.1936284934795\n",
        "(train): 2868.5497028355776."
      ],
      "metadata": {
        "id": "prL_NRnQYA7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2: Model fitting and prediction for Ridge Regression [20 POINTS]**\n",
        "\n",
        "In this implementation, we define a RidgeRegression class with a lambda parameter that controls the strength of regularization. In the fit method, we first add a bias term to the input data and then calculate the ridge regression coefficients using the closed-form solution:\n",
        "\n",
        "$$ \\widehat{\\beta} = ({X}^T {X} + \\lambda {I})^{-1} {X}^T {y}$$\n",
        "\n",
        "where $X$ is the input data with the bias term added, $y$ is the output vector, and $\\lambda$ is the regularization parameter.\n",
        "\n",
        "In this exercise we do not split $\\beta$ into <bias> and ```bias``` and ```weights``` as seen in earlier example. We consider ```betas``` as a single array of all model parameters $\\beta_0 . . . . \\beta_p$"
      ],
      "metadata": {
        "id": "mZiiXPNRRpY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RidgeRegression:\n",
        "    def __init__(self, lambdaa=0.1):\n",
        "        self.lambdaa = lambdaa\n",
        "        self.betas = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # compute the closed-form solution\n",
        "        self.betas = np.linalg.inv(X.T @ X + self.lambdaa * np.eye(X.shape[1])) @ X.T @ y\n",
        "        \n",
        "    def predict(self, X):\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) \n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # make predictions using the learned betas\n",
        "        y_predicted = X @ self.betas\n",
        "\n",
        "        return y_predicted"
      ],
      "metadata": {
        "id": "MY3xyapCODyn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Create an instance of the RidgeRegression class using above class with lambda taking values [0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100].\n",
        "\n",
        "You can use some code from Q1"
      ],
      "metadata": {
        "id": "du8wng0t-Jl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FILL IN ### [5 POINTS]\n",
        "# Report the train and test error for each value of 8 lambdas\n",
        "# What interesting can you note from the errors as we increase the lambda value ?\n",
        "\n",
        "lr = RidgeRegression(100)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error (test): {mse}\")\n",
        "\n",
        "y_pred = lr.predict(X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(f\"Mean squared error (train): {mse}\")\n",
        "\n",
        "\"\"\"\n",
        "[0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "Mean squared error (test): 2900.1936284934795\n",
        "Mean squared error (train): 2868.5497028355776\n",
        "\n",
        "Mean squared error (test): 2899.6798139329367\n",
        "Mean squared error (train): 2868.55531515633\n",
        "\n",
        "Mean squared error (test): 2895.825094075345\n",
        "Mean squared error (train): 2868.9973493863567\n",
        "\n",
        "Mean squared error (test): 2882.3289155849816\n",
        "Mean squared error (train): 2879.1251823380258\n",
        "\n",
        "Mean squared error (test): 2856.810366181945\n",
        "Mean squared error (train): 2912.9733505914587\n",
        "\n",
        "Mean squared error (test): 3076.955131013431\n",
        "Mean squared error (train): 3387.9020630481946\n",
        "\n",
        "Mean squared error (test): 4408.328454539778\n",
        "Mean squared error (train): 5099.249670617114\n",
        "\n",
        "Mean squared error (test): 5857.8051852382305\n",
        "Mean squared error (train): 7088.800187165489\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "bQohbqtfW1UR",
        "outputId": "9bda6d64-9bc1-44de-be2d-eb14858d6301"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error (test): 5857.8051852382305\n",
            "Mean squared error (train): 7088.800187165489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n[0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\\n\\nMean squared error (test): 2900.1936284934795\\nMean squared error (train): 2868.5497028355776\\n\\nMean squared error (test): 2899.6798139329367\\nMean squared error (train): 2868.55531515633\\n\\nMean squared error (test): 2895.825094075345\\nMean squared error (train): 2868.9973493863567\\n\\nMean squared error (test): 2882.3289155849816\\nMean squared error (train): 2879.1251823380258\\n\\nMean squared error (test): 2856.810366181945\\nMean squared error (train): 2912.9733505914587\\n\\nMean squared error (test): 3076.955131013431\\nMean squared error (train): 3387.9020630481946\\n\\nMean squared error (test): 4408.328454539778\\nMean squared error (train): 5099.249670617114\\n\\nMean squared error (test): 5857.8051852382305\\nMean squared error (train): 7088.800187165489\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: As lambda increases we get decreased weights and hence decreased influence from each feature."
      ],
      "metadata": {
        "id": "Lio3nF_BBwr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"### FILL IN ###\" [5 POINTS]\n",
        "\n",
        "Run the code found below for 8 different lambdas. \n",
        "\n",
        "Is there anything interesting that you can note about the model parameters ```betas``` by varying lambda ? \n",
        "\n",
        "Answer: Sparser model as lambda increases. 4 out of 10 betas diminsh when lambda is 100."
      ],
      "metadata": {
        "id": "nhLTsf7Ab-p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "for i in [0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:\n",
        "    lr = RidgeRegression(i)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print(i, \"\\n\",np.round(lr.betas,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBx_aSymTJLh",
        "outputId": "dcc5c546-74b0-4881-be8c-63ddbfc5d676"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \n",
            " [ 151.35   37.9  -241.96  542.43  347.7  -931.49  518.06  163.42  275.32\n",
            "  736.2    48.67]\n",
            "0.0001 \n",
            " [ 151.35   37.97 -241.9   542.59  347.6  -919.86  509.    158.21  273.69\n",
            "  731.71   48.79]\n",
            "0.001 \n",
            " [ 151.34   38.48 -241.35  543.84  346.78 -827.7   437.17  116.95  260.76\n",
            "  696.13   49.74]\n",
            "0.01 \n",
            " [ 151.33   40.7  -237.01  546.16  341.81 -430.15  129.9   -60.46  203.99\n",
            "  541.1    55.49]\n",
            "0.1 \n",
            " [ 151.42   42.88 -205.5   505.11  317.1  -108.51  -86.26 -190.37  151.7\n",
            "  392.3    79.93]\n",
            "1 \n",
            " [ 151.81   45.47  -76.68  291.45  199.04   -0.6   -28.67 -144.56  119.25\n",
            "  230.28  112.27]\n",
            "10 \n",
            " [149.13  18.42  -3.26  65.35  48.36  16.15  10.74 -39.33  41.47  58.1\n",
            "  39.6 ]\n",
            "100 \n",
            " [119.76   2.58   0.06   7.89   5.92   2.42   1.8   -4.95   5.44   7.21\n",
            "   5.25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3: Model fitting and prediction for LASSO Regression [25 POINTS]**\n",
        "\n",
        "In general, the LASSO lacks a closed form solution because the objective function is not differentiable.\n",
        "\n",
        "This implementation uses scikit-learn's Lasso class, which applies L1 regularization to the model parameters. The alpha parameter is responsible for regularization."
      ],
      "metadata": {
        "id": "4CF39mlJdInm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "FpmhDntbfglu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Create an instance of the Lasso class using above imported library with lambda taking values [0.0001, 0.001, 0.01, 0.1, 1, 10, 100].\n",
        "\n",
        "You can use some code from Q1"
      ],
      "metadata": {
        "id": "Efxi3gHD9hNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FILL IN ### [5 POINTS]\n",
        "# Report the train and test error for each value of 7 lambdas\n",
        "# What interesting can you note from the errors as we increase the lambda value ?\n",
        "\n",
        "lr = Lasso(100)\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean squared error (test): {mse}\")\n",
        "\n",
        "y_pred = lr.predict(X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(f\"Mean squared error (train): {mse}\")\n",
        "\n",
        "\"\"\"\n",
        "Mean squared error (test): 2899.803356085326\n",
        "Mean squared error (train): 2868.5515823952846\n",
        "\n",
        "Mean squared error (test): 2896.4097872031953\n",
        "Mean squared error (train): 2868.737582663929\n",
        "\n",
        "Mean squared error (test): 2878.559386186607\n",
        "Mean squared error (train): 2877.0810201119007\n",
        "\n",
        "Mean squared error (test): 2798.193485169719\n",
        "Mean squared error (train): 2935.25823259759\n",
        "\n",
        "Mean squared error (test): 3403.5757216070733\n",
        "Mean squared error (train): 3860.7549830123576\n",
        "\n",
        "Mean squared error (test): 5361.533457238513\n",
        "Mean squared error (train): 6076.398012984615\n",
        "\n",
        "Mean squared error (test): 5361.533457238513\n",
        "Mean squared error (train): 6076.398012984615\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "-jYLxjOPfW-8",
        "outputId": "5e792202-cb5a-4a93-e405-23c984b077e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error (test): 5361.533457238513\n",
            "Mean squared error (train): 6076.398012984615\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMean squared error (test): 2899.803356085326\\nMean squared error (train): 2868.5515823952846\\n\\nMean squared error (test): 2896.4097872031953\\nMean squared error (train): 2868.737582663929\\n\\nMean squared error (test): 2878.559386186607\\nMean squared error (train): 2877.0810201119007\\n\\nMean squared error (test): 2798.193485169719\\nMean squared error (train): 2935.25823259759\\n\\nMean squared error (test): 3403.5757216070733\\nMean squared error (train): 3860.7549830123576\\n\\nMean squared error (test): 5361.533457238513\\nMean squared error (train): 6076.398012984615\\n\\nMean squared error (test): 5361.533457238513\\nMean squared error (train): 6076.398012984615\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: \n",
        "\n",
        "\n",
        "1) As lambda increases, the the train error increases.\n",
        "2) As lambda increase, the test error initially decreases upto certain points and increases.\n",
        "\n",
        "From this we can understand that increasing lambda leads to higher regularization. This results to simpler model which is less prone to overfitting the training data. "
      ],
      "metadata": {
        "id": "HURZKZQrBibm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"### FILL IN ###\" [5 POINTS]\n",
        "\n",
        "Run the code found below for 7 different lambdas. \n",
        "\n",
        "Is there anything interesting that you can note about the model parameters ```betas``` by varying lambda ? \n",
        "\n",
        "Answer: Sparser model as lambda increases. all 10 betas diminsh when lambda reaches 10."
      ],
      "metadata": {
        "id": "zKcmkte0fE7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### report the beta values\n",
        "for i in [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]:\n",
        "    lr = Lasso(i)\n",
        "    lr.fit(X_train, y_train)\n",
        "    print(i, \"\\n\",np.round(lr.coef_,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aICQZXTPeEfN",
        "outputId": "7f5261ee-4c87-4e72-c2cd-4657b877f06e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 \n",
            " [  37.89 -241.88  542.52  347.62 -924.83  513.03  160.22  274.04  733.76\n",
            "   48.69]\n",
            "0.001 \n",
            " [  37.78 -241.1   543.39  346.91 -864.93  467.78  131.47  262.5   711.84\n",
            "   48.86]\n",
            "0.01 \n",
            " [  35.35 -231.91  551.3   341.31 -483.57  165.99  -25.83  216.88  566.82\n",
            "   49.7 ]\n",
            "0.1 \n",
            " [   0.   -152.66  552.7   303.37  -81.37   -0.   -229.26    0.    447.92\n",
            "   29.64]\n",
            "1 \n",
            " [  0.    -0.   413.43  34.83   0.     0.    -0.     0.   258.15   0.  ]\n",
            "10 \n",
            " [ 0.  0.  0.  0.  0.  0. -0.  0.  0.  0.]\n",
            "100 \n",
            " [ 0.  0.  0.  0.  0.  0. -0.  0.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"### FILL IN ###\" [3 POINTS]\n",
        "\n",
        "How do you decide whether your linear regression model fits the data?\n",
        "\n",
        "Answer: Using an evaluation metric (MSE, RMSE, MAE) on the hold out set and not on the training set.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\"### FILL IN ###\" [3 POINTS]\n",
        "\n",
        "In LASSO, what happens to the $i^{th}$ feature if its corresponding $\\beta_i$ is zero ? \n",
        "\n",
        "Answer: Sparsity due to lesser features (parameter selection). Features are eliminated.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\"### FILL IN ###\" [3 POINTS]\n",
        "\n",
        "In LASSO, what happens to overall model in terms of complexity as compared to unregularized Linear Regression ? \n",
        "\n",
        "Answer: sparser model as few features zero so its simpler model.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\"### FILL IN ###\" [3 POINTS]\n",
        "\n",
        "What happens in Ridge regression when $\\lambda$ is equal to zero ? \n",
        "\n",
        "Answer: Same as basic linear regression\n",
        "\n",
        "<hr>\n",
        "\n",
        "\"### FILL IN ###\" [3 POINTS]\n",
        "\n",
        "Which regularization technique achieves both parameter selection and shrinkage ?\n",
        "\n",
        "Answer: LASSO\n",
        "\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "lxACpDOXgJzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART B: NEURAL NETWORKS [40 POINTS]**"
      ],
      "metadata": {
        "id": "88XRwadTVF56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hyperparameters in a neural network model are parameters that are set prior to the training of the model and are not learned during the training process. They affect the behavior and performance of the model. The process of selecting the optimal hyperparameters is called hyperparameter tuning and is an important part of building an effective machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "Here are some common hyperparameters in a neural network model:\n",
        "\n",
        "* Learning rate: This hyperparameter controls the step size of the optimizer during training. \n",
        "\n",
        "* Number of hidden layers: The number of hidden layers in a neural network model is a hyperparameter that determines the depth of the network. \n",
        "\n",
        "* Number of neurons per layer: This hyperparameter determines the width of the neural network model. \n",
        "\n",
        "* Activation function: The activation function is applied to the output of each neuron in a neural network model. Different activation functions can have different properties, such as being more or less sensitive to input or output values. Popular activation functions include ReLU, sigmoid, and tanh."
      ],
      "metadata": {
        "id": "W98UPuWDBT0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: Neural Network on Iris Dataset [20 POINTS]**\n",
        "\n",
        "In this example, we're normalizing the input features to have zero mean and unit variance, which can help with training. We're also one-hot encoding the target variable, which is a common technique for training classification models."
      ],
      "metadata": {
        "id": "qk8u5SMS4H90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "tfeIcwiF5Uuz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### looking at first 3 rows of dataset\n",
        "\n",
        "print(iris.data[:3])\n",
        "print(\"\\n\")\n",
        "print(iris.target[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO_EXY1g5jWk",
        "outputId": "a7714a20-4e63-4963-a4fb-23cdefe891e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]]\n",
            "\n",
            "\n",
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: In this exercise we will use our test set as our validation set for finding the optimal value of our hyperparameters(as discussed in the class). Assume that the real testing dataset which we use to report the final model performance is unknown to us. In real life machine learning you do not have access to test data."
      ],
      "metadata": {
        "id": "Y3g-aLyfOOiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# normalize input features\n",
        "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "X_test = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "\n",
        "# one-hot encode target variable\n",
        "y_train_onehot = np.zeros((len(y_train), 3))\n",
        "y_train_onehot[np.arange(len(y_train)), y_train] = 1\n",
        "y_test_onehot = np.zeros((len(y_test), 3))\n",
        "y_test_onehot[np.arange(len(y_test)), y_test] = 1"
      ],
      "metadata": {
        "id": "iuAb5KAxVJCC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Us0p7f8TbAP",
        "outputId": "190388b4-331f-4b97-c2bc-bb767ca53c18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 4), (150,))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train_onehot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epFvhpKLXPoX",
        "outputId": "a6a4af79-8afe-4338-9f93-45d10bb26a90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120, 4), (120, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape, y_test_onehot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCpqwbRCXTir",
        "outputId": "ec20942c-5f04-4e93-bfa2-6ecc1917a231"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 4), (30, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, we define the NeuralNetwork class which takes four arguments as input: \n",
        "* input_size\n",
        "* hidden_size\n",
        "* output_size\n",
        "* learning_rate\n",
        "\n",
        "The forward() method performs forward propagation through the network\n",
        "\n",
        "The backward() method performs backpropagation to compute gradients and update weights and biases\n",
        "\n",
        "The train() method trains the network using the specified learning rate and number of epochs.\n",
        "\n",
        "The sigmoid() method implements the sigmoid activation function, and the sigmoid_derivative() method implements the derivative of the sigmoid function, which is used in the backpropagation algorithm."
      ],
      "metadata": {
        "id": "oA7_zQww36y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # Initialize weights and biases\n",
        "        np.random.seed(42)\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "        np.random.seed(21)\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        sigmoid_output = 1 / (1 + np.exp(-z))\n",
        "        return sigmoid_output\n",
        "    \n",
        "    def sigmoid_derivative(self, z):\n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        sigmoid_derivative_output = self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "        return sigmoid_derivative_output\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # Forward propagation\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        \n",
        "        return self.a2\n",
        "    \n",
        "    def backward(self, X, y, output):\n",
        "        # Backward propagation\n",
        "        self.error = y - output\n",
        "        self.delta2 = self.error * self.sigmoid_derivative(self.z2)\n",
        "        self.dW2 = np.dot(self.a1.T, self.delta2)\n",
        "        self.db2 = np.sum(self.delta2, axis=0, keepdims=True)\n",
        "        self.delta1 = np.dot(self.delta2, self.W2.T) * self.sigmoid_derivative(self.z1)\n",
        "        self.dW1 = np.dot(X.T, self.delta1)\n",
        "        self.db1 = np.sum(self.delta1, axis=0)\n",
        "        \n",
        "        ### FILL IN ### [5 POINTS]\n",
        "        # Update weights and biases using the derivatives\n",
        "        self.W2 += self.learning_rate * self.dW2\n",
        "        self.b2 += self.learning_rate * self.db2\n",
        "        self.W1 += self.learning_rate * self.dW1\n",
        "        self.b1 += self.learning_rate * self.db1\n",
        "    \n",
        "    def train(self, X, y, X_test, y_test, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, output)\n",
        "            loss = np.mean(np.square(y - output))\n",
        "\n",
        "            # evaluate on test set\n",
        "            test_output = self.forward(X_test)\n",
        "            test_loss = np.mean(np.abs(y_test - test_output))\n",
        "            #test_accuracy = np.mean(np.argmax(test_output, axis=1) == y_test)\n",
        "\n",
        "            if epoch%10==0:\n",
        "                print(\"Epoch %d - train_loss: %.4f , test_loss: %.4f\" % (epoch, loss, test_loss))\n"
      ],
      "metadata": {
        "id": "OrCv8V2bBGv-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To use this neural network, we can create an instance of the NeuralNetwork class and train it on a dataset:"
      ],
      "metadata": {
        "id": "MrDZUq_I5JKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 3\n",
        "LR = 0.001\n",
        "EPOCHS = 500\n",
        "\n",
        "nn = NeuralNetwork(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE, learning_rate=LR)"
      ],
      "metadata": {
        "id": "FPJ0SDwPTlMY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.train(X=X_train, y=y_train_onehot, X_test=X_test, y_test=y_test_onehot, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUef4PY-UISE",
        "outputId": "db52744e-b4b8-4008-bf69-07f74ef528f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4980\n",
            "Epoch 10 - train_loss: 0.2383 , test_loss: 0.4857\n",
            "Epoch 20 - train_loss: 0.2318 , test_loss: 0.4764\n",
            "Epoch 30 - train_loss: 0.2280 , test_loss: 0.4694\n",
            "Epoch 40 - train_loss: 0.2257 , test_loss: 0.4641\n",
            "Epoch 50 - train_loss: 0.2244 , test_loss: 0.4600\n",
            "Epoch 60 - train_loss: 0.2235 , test_loss: 0.4568\n",
            "Epoch 70 - train_loss: 0.2230 , test_loss: 0.4544\n",
            "Epoch 80 - train_loss: 0.2227 , test_loss: 0.4525\n",
            "Epoch 90 - train_loss: 0.2225 , test_loss: 0.4510\n",
            "Epoch 100 - train_loss: 0.2224 , test_loss: 0.4498\n",
            "Epoch 110 - train_loss: 0.2223 , test_loss: 0.4488\n",
            "Epoch 120 - train_loss: 0.2222 , test_loss: 0.4481\n",
            "Epoch 130 - train_loss: 0.2222 , test_loss: 0.4475\n",
            "Epoch 140 - train_loss: 0.2222 , test_loss: 0.4470\n",
            "Epoch 150 - train_loss: 0.2222 , test_loss: 0.4466\n",
            "Epoch 160 - train_loss: 0.2221 , test_loss: 0.4463\n",
            "Epoch 170 - train_loss: 0.2221 , test_loss: 0.4460\n",
            "Epoch 180 - train_loss: 0.2221 , test_loss: 0.4458\n",
            "Epoch 190 - train_loss: 0.2221 , test_loss: 0.4457\n",
            "Epoch 200 - train_loss: 0.2221 , test_loss: 0.4455\n",
            "Epoch 210 - train_loss: 0.2221 , test_loss: 0.4454\n",
            "Epoch 220 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 230 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 240 - train_loss: 0.2221 , test_loss: 0.4452\n",
            "Epoch 250 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 260 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 270 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 280 - train_loss: 0.2220 , test_loss: 0.4450\n",
            "Epoch 290 - train_loss: 0.2220 , test_loss: 0.4450\n",
            "Epoch 300 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 310 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 320 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 330 - train_loss: 0.2218 , test_loss: 0.4448\n",
            "Epoch 340 - train_loss: 0.2218 , test_loss: 0.4448\n",
            "Epoch 350 - train_loss: 0.2217 , test_loss: 0.4447\n",
            "Epoch 360 - train_loss: 0.2217 , test_loss: 0.4446\n",
            "Epoch 370 - train_loss: 0.2216 , test_loss: 0.4446\n",
            "Epoch 380 - train_loss: 0.2215 , test_loss: 0.4445\n",
            "Epoch 390 - train_loss: 0.2214 , test_loss: 0.4444\n",
            "Epoch 400 - train_loss: 0.2213 , test_loss: 0.4443\n",
            "Epoch 410 - train_loss: 0.2212 , test_loss: 0.4442\n",
            "Epoch 420 - train_loss: 0.2211 , test_loss: 0.4441\n",
            "Epoch 430 - train_loss: 0.2210 , test_loss: 0.4439\n",
            "Epoch 440 - train_loss: 0.2208 , test_loss: 0.4438\n",
            "Epoch 450 - train_loss: 0.2206 , test_loss: 0.4436\n",
            "Epoch 460 - train_loss: 0.2204 , test_loss: 0.4434\n",
            "Epoch 470 - train_loss: 0.2202 , test_loss: 0.4432\n",
            "Epoch 480 - train_loss: 0.2199 , test_loss: 0.4429\n",
            "Epoch 490 - train_loss: 0.2196 , test_loss: 0.4427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5: What if we vary the hyper parameters [20 POINTS]**"
      ],
      "metadata": {
        "id": "SimtQgge8j5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### FILL IN ### [4 POINTS]\n",
        "# Report both the losses after 500 epochs for each of the 7 different values of learning rate. \n",
        "# Is there anything interesting that you note when you increase the learning rate ?\n",
        "\n",
        "INPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 3\n",
        "EPOCHS = 500\n",
        "\n",
        "\n",
        "for lr in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 5]:\n",
        "    print(\"*\"*30)\n",
        "    print(\"PROCEDURE FOR LR = \", lr)\n",
        "    nn = NeuralNetwork(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE, learning_rate=lr)\n",
        "    nn.train(X=X_train, y=y_train_onehot, X_test=X_test, y_test=y_test_onehot, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zrtuFlNRV29",
        "outputId": "461cc26d-ee84-4b8b-f38a-e10f92b3a2f4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************\n",
            "PROCEDURE FOR LR =  1e-06\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 10 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 20 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 30 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 40 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 50 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 60 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 70 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 80 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 90 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 100 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 110 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 120 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 130 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 140 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 150 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 160 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 170 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 180 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 190 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 200 - train_loss: 0.2493 , test_loss: 0.4992\n",
            "Epoch 210 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 220 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 230 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 240 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 250 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 260 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 270 - train_loss: 0.2492 , test_loss: 0.4991\n",
            "Epoch 280 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 290 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 300 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 310 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 320 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 330 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 340 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 350 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 360 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 370 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 380 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 390 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 400 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 410 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 420 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 430 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 440 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 450 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 460 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 470 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 480 - train_loss: 0.2489 , test_loss: 0.4988\n",
            "Epoch 490 - train_loss: 0.2488 , test_loss: 0.4987\n",
            "******************************\n",
            "PROCEDURE FOR LR =  1e-05\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4994\n",
            "Epoch 10 - train_loss: 0.2494 , test_loss: 0.4993\n",
            "Epoch 20 - train_loss: 0.2493 , test_loss: 0.4991\n",
            "Epoch 30 - train_loss: 0.2491 , test_loss: 0.4990\n",
            "Epoch 40 - train_loss: 0.2490 , test_loss: 0.4989\n",
            "Epoch 50 - train_loss: 0.2488 , test_loss: 0.4987\n",
            "Epoch 60 - train_loss: 0.2487 , test_loss: 0.4986\n",
            "Epoch 70 - train_loss: 0.2486 , test_loss: 0.4984\n",
            "Epoch 80 - train_loss: 0.2484 , test_loss: 0.4983\n",
            "Epoch 90 - train_loss: 0.2483 , test_loss: 0.4982\n",
            "Epoch 100 - train_loss: 0.2481 , test_loss: 0.4980\n",
            "Epoch 110 - train_loss: 0.2480 , test_loss: 0.4979\n",
            "Epoch 120 - train_loss: 0.2479 , test_loss: 0.4977\n",
            "Epoch 130 - train_loss: 0.2477 , test_loss: 0.4976\n",
            "Epoch 140 - train_loss: 0.2476 , test_loss: 0.4975\n",
            "Epoch 150 - train_loss: 0.2475 , test_loss: 0.4973\n",
            "Epoch 160 - train_loss: 0.2473 , test_loss: 0.4972\n",
            "Epoch 170 - train_loss: 0.2472 , test_loss: 0.4970\n",
            "Epoch 180 - train_loss: 0.2471 , test_loss: 0.4969\n",
            "Epoch 190 - train_loss: 0.2469 , test_loss: 0.4968\n",
            "Epoch 200 - train_loss: 0.2468 , test_loss: 0.4966\n",
            "Epoch 210 - train_loss: 0.2467 , test_loss: 0.4965\n",
            "Epoch 220 - train_loss: 0.2466 , test_loss: 0.4964\n",
            "Epoch 230 - train_loss: 0.2464 , test_loss: 0.4962\n",
            "Epoch 240 - train_loss: 0.2463 , test_loss: 0.4961\n",
            "Epoch 250 - train_loss: 0.2462 , test_loss: 0.4960\n",
            "Epoch 260 - train_loss: 0.2461 , test_loss: 0.4958\n",
            "Epoch 270 - train_loss: 0.2459 , test_loss: 0.4957\n",
            "Epoch 280 - train_loss: 0.2458 , test_loss: 0.4956\n",
            "Epoch 290 - train_loss: 0.2457 , test_loss: 0.4954\n",
            "Epoch 300 - train_loss: 0.2456 , test_loss: 0.4953\n",
            "Epoch 310 - train_loss: 0.2454 , test_loss: 0.4952\n",
            "Epoch 320 - train_loss: 0.2453 , test_loss: 0.4950\n",
            "Epoch 330 - train_loss: 0.2452 , test_loss: 0.4949\n",
            "Epoch 340 - train_loss: 0.2451 , test_loss: 0.4948\n",
            "Epoch 350 - train_loss: 0.2450 , test_loss: 0.4946\n",
            "Epoch 360 - train_loss: 0.2448 , test_loss: 0.4945\n",
            "Epoch 370 - train_loss: 0.2447 , test_loss: 0.4944\n",
            "Epoch 380 - train_loss: 0.2446 , test_loss: 0.4943\n",
            "Epoch 390 - train_loss: 0.2445 , test_loss: 0.4941\n",
            "Epoch 400 - train_loss: 0.2444 , test_loss: 0.4940\n",
            "Epoch 410 - train_loss: 0.2442 , test_loss: 0.4939\n",
            "Epoch 420 - train_loss: 0.2441 , test_loss: 0.4937\n",
            "Epoch 430 - train_loss: 0.2440 , test_loss: 0.4936\n",
            "Epoch 440 - train_loss: 0.2439 , test_loss: 0.4935\n",
            "Epoch 450 - train_loss: 0.2438 , test_loss: 0.4934\n",
            "Epoch 460 - train_loss: 0.2437 , test_loss: 0.4932\n",
            "Epoch 470 - train_loss: 0.2436 , test_loss: 0.4931\n",
            "Epoch 480 - train_loss: 0.2435 , test_loss: 0.4930\n",
            "Epoch 490 - train_loss: 0.2433 , test_loss: 0.4929\n",
            "******************************\n",
            "PROCEDURE FOR LR =  0.0001\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4993\n",
            "Epoch 10 - train_loss: 0.2481 , test_loss: 0.4979\n",
            "Epoch 20 - train_loss: 0.2468 , test_loss: 0.4965\n",
            "Epoch 30 - train_loss: 0.2456 , test_loss: 0.4952\n",
            "Epoch 40 - train_loss: 0.2444 , test_loss: 0.4939\n",
            "Epoch 50 - train_loss: 0.2432 , test_loss: 0.4926\n",
            "Epoch 60 - train_loss: 0.2422 , test_loss: 0.4914\n",
            "Epoch 70 - train_loss: 0.2411 , test_loss: 0.4902\n",
            "Epoch 80 - train_loss: 0.2402 , test_loss: 0.4890\n",
            "Epoch 90 - train_loss: 0.2393 , test_loss: 0.4879\n",
            "Epoch 100 - train_loss: 0.2384 , test_loss: 0.4868\n",
            "Epoch 110 - train_loss: 0.2376 , test_loss: 0.4857\n",
            "Epoch 120 - train_loss: 0.2368 , test_loss: 0.4847\n",
            "Epoch 130 - train_loss: 0.2361 , test_loss: 0.4837\n",
            "Epoch 140 - train_loss: 0.2354 , test_loss: 0.4827\n",
            "Epoch 150 - train_loss: 0.2347 , test_loss: 0.4817\n",
            "Epoch 160 - train_loss: 0.2341 , test_loss: 0.4808\n",
            "Epoch 170 - train_loss: 0.2335 , test_loss: 0.4799\n",
            "Epoch 180 - train_loss: 0.2329 , test_loss: 0.4790\n",
            "Epoch 190 - train_loss: 0.2324 , test_loss: 0.4782\n",
            "Epoch 200 - train_loss: 0.2319 , test_loss: 0.4773\n",
            "Epoch 210 - train_loss: 0.2314 , test_loss: 0.4765\n",
            "Epoch 220 - train_loss: 0.2309 , test_loss: 0.4757\n",
            "Epoch 230 - train_loss: 0.2305 , test_loss: 0.4750\n",
            "Epoch 240 - train_loss: 0.2301 , test_loss: 0.4742\n",
            "Epoch 250 - train_loss: 0.2297 , test_loss: 0.4735\n",
            "Epoch 260 - train_loss: 0.2293 , test_loss: 0.4728\n",
            "Epoch 270 - train_loss: 0.2290 , test_loss: 0.4721\n",
            "Epoch 280 - train_loss: 0.2287 , test_loss: 0.4714\n",
            "Epoch 290 - train_loss: 0.2284 , test_loss: 0.4708\n",
            "Epoch 300 - train_loss: 0.2281 , test_loss: 0.4702\n",
            "Epoch 310 - train_loss: 0.2278 , test_loss: 0.4695\n",
            "Epoch 320 - train_loss: 0.2275 , test_loss: 0.4689\n",
            "Epoch 330 - train_loss: 0.2273 , test_loss: 0.4684\n",
            "Epoch 340 - train_loss: 0.2270 , test_loss: 0.4678\n",
            "Epoch 350 - train_loss: 0.2268 , test_loss: 0.4672\n",
            "Epoch 360 - train_loss: 0.2266 , test_loss: 0.4667\n",
            "Epoch 370 - train_loss: 0.2264 , test_loss: 0.4662\n",
            "Epoch 380 - train_loss: 0.2262 , test_loss: 0.4657\n",
            "Epoch 390 - train_loss: 0.2260 , test_loss: 0.4652\n",
            "Epoch 400 - train_loss: 0.2258 , test_loss: 0.4647\n",
            "Epoch 410 - train_loss: 0.2256 , test_loss: 0.4642\n",
            "Epoch 420 - train_loss: 0.2255 , test_loss: 0.4638\n",
            "Epoch 430 - train_loss: 0.2253 , test_loss: 0.4633\n",
            "Epoch 440 - train_loss: 0.2252 , test_loss: 0.4629\n",
            "Epoch 450 - train_loss: 0.2250 , test_loss: 0.4625\n",
            "Epoch 460 - train_loss: 0.2249 , test_loss: 0.4621\n",
            "Epoch 470 - train_loss: 0.2248 , test_loss: 0.4617\n",
            "Epoch 480 - train_loss: 0.2246 , test_loss: 0.4613\n",
            "Epoch 490 - train_loss: 0.2245 , test_loss: 0.4609\n",
            "******************************\n",
            "PROCEDURE FOR LR =  0.001\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4980\n",
            "Epoch 10 - train_loss: 0.2383 , test_loss: 0.4857\n",
            "Epoch 20 - train_loss: 0.2318 , test_loss: 0.4764\n",
            "Epoch 30 - train_loss: 0.2280 , test_loss: 0.4694\n",
            "Epoch 40 - train_loss: 0.2257 , test_loss: 0.4641\n",
            "Epoch 50 - train_loss: 0.2244 , test_loss: 0.4600\n",
            "Epoch 60 - train_loss: 0.2235 , test_loss: 0.4568\n",
            "Epoch 70 - train_loss: 0.2230 , test_loss: 0.4544\n",
            "Epoch 80 - train_loss: 0.2227 , test_loss: 0.4525\n",
            "Epoch 90 - train_loss: 0.2225 , test_loss: 0.4510\n",
            "Epoch 100 - train_loss: 0.2224 , test_loss: 0.4498\n",
            "Epoch 110 - train_loss: 0.2223 , test_loss: 0.4488\n",
            "Epoch 120 - train_loss: 0.2222 , test_loss: 0.4481\n",
            "Epoch 130 - train_loss: 0.2222 , test_loss: 0.4475\n",
            "Epoch 140 - train_loss: 0.2222 , test_loss: 0.4470\n",
            "Epoch 150 - train_loss: 0.2222 , test_loss: 0.4466\n",
            "Epoch 160 - train_loss: 0.2221 , test_loss: 0.4463\n",
            "Epoch 170 - train_loss: 0.2221 , test_loss: 0.4460\n",
            "Epoch 180 - train_loss: 0.2221 , test_loss: 0.4458\n",
            "Epoch 190 - train_loss: 0.2221 , test_loss: 0.4457\n",
            "Epoch 200 - train_loss: 0.2221 , test_loss: 0.4455\n",
            "Epoch 210 - train_loss: 0.2221 , test_loss: 0.4454\n",
            "Epoch 220 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 230 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 240 - train_loss: 0.2221 , test_loss: 0.4452\n",
            "Epoch 250 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 260 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 270 - train_loss: 0.2220 , test_loss: 0.4451\n",
            "Epoch 280 - train_loss: 0.2220 , test_loss: 0.4450\n",
            "Epoch 290 - train_loss: 0.2220 , test_loss: 0.4450\n",
            "Epoch 300 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 310 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 320 - train_loss: 0.2219 , test_loss: 0.4449\n",
            "Epoch 330 - train_loss: 0.2218 , test_loss: 0.4448\n",
            "Epoch 340 - train_loss: 0.2218 , test_loss: 0.4448\n",
            "Epoch 350 - train_loss: 0.2217 , test_loss: 0.4447\n",
            "Epoch 360 - train_loss: 0.2217 , test_loss: 0.4446\n",
            "Epoch 370 - train_loss: 0.2216 , test_loss: 0.4446\n",
            "Epoch 380 - train_loss: 0.2215 , test_loss: 0.4445\n",
            "Epoch 390 - train_loss: 0.2214 , test_loss: 0.4444\n",
            "Epoch 400 - train_loss: 0.2213 , test_loss: 0.4443\n",
            "Epoch 410 - train_loss: 0.2212 , test_loss: 0.4442\n",
            "Epoch 420 - train_loss: 0.2211 , test_loss: 0.4441\n",
            "Epoch 430 - train_loss: 0.2210 , test_loss: 0.4439\n",
            "Epoch 440 - train_loss: 0.2208 , test_loss: 0.4438\n",
            "Epoch 450 - train_loss: 0.2206 , test_loss: 0.4436\n",
            "Epoch 460 - train_loss: 0.2204 , test_loss: 0.4434\n",
            "Epoch 470 - train_loss: 0.2202 , test_loss: 0.4432\n",
            "Epoch 480 - train_loss: 0.2199 , test_loss: 0.4429\n",
            "Epoch 490 - train_loss: 0.2196 , test_loss: 0.4427\n",
            "******************************\n",
            "PROCEDURE FOR LR =  0.01\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "Epoch 250 - train_loss: 0.0945 , test_loss: 0.4284\n",
            "Epoch 260 - train_loss: 0.0932 , test_loss: 0.4237\n",
            "Epoch 270 - train_loss: 0.0918 , test_loss: 0.4184\n",
            "Epoch 280 - train_loss: 0.0904 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0889 , test_loss: 0.4062\n",
            "Epoch 300 - train_loss: 0.0873 , test_loss: 0.3994\n",
            "Epoch 310 - train_loss: 0.0856 , test_loss: 0.3924\n",
            "Epoch 320 - train_loss: 0.0838 , test_loss: 0.3853\n",
            "Epoch 330 - train_loss: 0.0819 , test_loss: 0.3784\n",
            "Epoch 340 - train_loss: 0.0799 , test_loss: 0.3717\n",
            "Epoch 350 - train_loss: 0.0779 , test_loss: 0.3653\n",
            "Epoch 360 - train_loss: 0.0758 , test_loss: 0.3594\n",
            "Epoch 370 - train_loss: 0.0737 , test_loss: 0.3541\n",
            "Epoch 380 - train_loss: 0.0716 , test_loss: 0.3494\n",
            "Epoch 390 - train_loss: 0.0696 , test_loss: 0.3453\n",
            "Epoch 400 - train_loss: 0.0675 , test_loss: 0.3418\n",
            "Epoch 410 - train_loss: 0.0655 , test_loss: 0.3388\n",
            "Epoch 420 - train_loss: 0.0636 , test_loss: 0.3365\n",
            "Epoch 430 - train_loss: 0.0616 , test_loss: 0.3346\n",
            "Epoch 440 - train_loss: 0.0597 , test_loss: 0.3332\n",
            "Epoch 450 - train_loss: 0.0579 , test_loss: 0.3322\n",
            "Epoch 460 - train_loss: 0.0560 , test_loss: 0.3315\n",
            "Epoch 470 - train_loss: 0.0542 , test_loss: 0.3310\n",
            "Epoch 480 - train_loss: 0.0525 , test_loss: 0.3308\n",
            "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
            "******************************\n",
            "PROCEDURE FOR LR =  0.1\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.3837\n",
            "Epoch 10 - train_loss: 0.1799 , test_loss: 0.4369\n",
            "Epoch 20 - train_loss: 0.1102 , test_loss: 0.4645\n",
            "Epoch 30 - train_loss: 0.0913 , test_loss: 0.4300\n",
            "Epoch 40 - train_loss: 0.0766 , test_loss: 0.3562\n",
            "Epoch 50 - train_loss: 0.0610 , test_loss: 0.3231\n",
            "Epoch 60 - train_loss: 0.0425 , test_loss: 0.3258\n",
            "Epoch 70 - train_loss: 0.0290 , test_loss: 0.3573\n",
            "Epoch 80 - train_loss: 0.0225 , test_loss: 0.3783\n",
            "Epoch 90 - train_loss: 0.0193 , test_loss: 0.3896\n",
            "Epoch 100 - train_loss: 0.0173 , test_loss: 0.3958\n",
            "Epoch 110 - train_loss: 0.0160 , test_loss: 0.3996\n",
            "Epoch 120 - train_loss: 0.0150 , test_loss: 0.4022\n",
            "Epoch 130 - train_loss: 0.0143 , test_loss: 0.4040\n",
            "Epoch 140 - train_loss: 0.0138 , test_loss: 0.4054\n",
            "Epoch 150 - train_loss: 0.0134 , test_loss: 0.4065\n",
            "Epoch 160 - train_loss: 0.0130 , test_loss: 0.4074\n",
            "Epoch 170 - train_loss: 0.0127 , test_loss: 0.4082\n",
            "Epoch 180 - train_loss: 0.0125 , test_loss: 0.4088\n",
            "Epoch 190 - train_loss: 0.0123 , test_loss: 0.4094\n",
            "Epoch 200 - train_loss: 0.0121 , test_loss: 0.4099\n",
            "Epoch 210 - train_loss: 0.0119 , test_loss: 0.4104\n",
            "Epoch 220 - train_loss: 0.0118 , test_loss: 0.4107\n",
            "Epoch 230 - train_loss: 0.0117 , test_loss: 0.4111\n",
            "Epoch 240 - train_loss: 0.0115 , test_loss: 0.4114\n",
            "Epoch 250 - train_loss: 0.0114 , test_loss: 0.4117\n",
            "Epoch 260 - train_loss: 0.0114 , test_loss: 0.4120\n",
            "Epoch 270 - train_loss: 0.0113 , test_loss: 0.4123\n",
            "Epoch 280 - train_loss: 0.0112 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0111 , test_loss: 0.4127\n",
            "Epoch 300 - train_loss: 0.0111 , test_loss: 0.4129\n",
            "Epoch 310 - train_loss: 0.0110 , test_loss: 0.4131\n",
            "Epoch 320 - train_loss: 0.0110 , test_loss: 0.4133\n",
            "Epoch 330 - train_loss: 0.0109 , test_loss: 0.4135\n",
            "Epoch 340 - train_loss: 0.0109 , test_loss: 0.4136\n",
            "Epoch 350 - train_loss: 0.0108 , test_loss: 0.4138\n",
            "Epoch 360 - train_loss: 0.0108 , test_loss: 0.4139\n",
            "Epoch 370 - train_loss: 0.0108 , test_loss: 0.4140\n",
            "Epoch 380 - train_loss: 0.0107 , test_loss: 0.4142\n",
            "Epoch 390 - train_loss: 0.0107 , test_loss: 0.4143\n",
            "Epoch 400 - train_loss: 0.0107 , test_loss: 0.4144\n",
            "Epoch 410 - train_loss: 0.0106 , test_loss: 0.4145\n",
            "Epoch 420 - train_loss: 0.0106 , test_loss: 0.4146\n",
            "Epoch 430 - train_loss: 0.0106 , test_loss: 0.4147\n",
            "Epoch 440 - train_loss: 0.0106 , test_loss: 0.4148\n",
            "Epoch 450 - train_loss: 0.0105 , test_loss: 0.4149\n",
            "Epoch 460 - train_loss: 0.0105 , test_loss: 0.4150\n",
            "Epoch 470 - train_loss: 0.0105 , test_loss: 0.4150\n",
            "Epoch 480 - train_loss: 0.0105 , test_loss: 0.4151\n",
            "Epoch 490 - train_loss: 0.0105 , test_loss: 0.4152\n",
            "******************************\n",
            "PROCEDURE FOR LR =  1\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.3333\n",
            "Epoch 10 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 20 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 30 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 40 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 50 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 60 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 70 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 80 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 90 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 100 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 110 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 120 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 130 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 140 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 150 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 160 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 170 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 180 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 190 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 200 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 210 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 220 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 230 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 240 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 250 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 260 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 270 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 280 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 290 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 300 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 310 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 320 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 330 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 340 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 350 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 360 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 370 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 380 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 390 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 400 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 410 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 420 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 430 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 440 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 450 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 460 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 470 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 480 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 490 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "******************************\n",
            "PROCEDURE FOR LR =  5\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.3333\n",
            "Epoch 10 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 20 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 30 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 40 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 50 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 60 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 70 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 80 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 90 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 100 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 110 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 120 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 130 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 140 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 150 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 160 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 170 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 180 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 190 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 200 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 210 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 220 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 230 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 240 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 250 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 260 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 270 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 280 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 290 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 300 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 310 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 320 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 330 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 340 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 350 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 360 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 370 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 380 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 390 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 400 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 410 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 420 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 430 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 440 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 450 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 460 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 470 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 480 - train_loss: 0.3333 , test_loss: 0.3333\n",
            "Epoch 490 - train_loss: 0.3333 , test_loss: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: No learning when LR>1"
      ],
      "metadata": {
        "id": "3mPHWgF3BUsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"### FILL IN ###\" [4 POINTS]\n",
        "\n",
        "What happens when the learning rate is too high, also what if it is too low?\n",
        "\n",
        "Answer: A high learning rate can cause the optimizer to overshoot the minimum, while a low learning rate can cause slow convergence or the model to get stuck in a local minimum.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\"### FILL IN ###\" [4 POINTS]\n",
        "\n",
        "Which one of the seven learning rates (0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 5) is the most optimal ?\n",
        "\n",
        "Answer: 0.01\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "\"### FILL IN ###\" [4 POINTS]\n",
        "\n",
        "State one advantage and one disadvantage of increasing the number of hidden layers and number of units in each hidden layer?\n",
        "\n",
        "Answer: Adding more hidden layers can make the model more expressive and able to learn more complex functions, but can also increase the risk of overfitting. Increasing the number of neurons per layer can make the model more powerful, but also increases the risk of overfitting.\n",
        "\n",
        "<hr>\n",
        "\n",
        "\n",
        "\"### FILL IN ###\" [4 POINTS]\n",
        "\n",
        "Using the most optimal learning rate, run the process for 100,250,500,750,1000,2000 epochs and report errors\n",
        "\n",
        "Answer: \n",
        "\n",
        "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
        "\n",
        "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
        "\n",
        "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
        "\n",
        "Epoch 740 - train_loss: 0.0240 , test_loss: 0.3877\n",
        "\n",
        "Epoch 990 - train_loss: 0.0170 , test_loss: 0.4067\n",
        "\n",
        "Epoch 1990 - train_loss: 0.0120 , test_loss: 0.4152\n",
        "\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "cy1wVnXWBkf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### for epoch calculations\n",
        "INPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 10\n",
        "OUTPUT_SIZE = 3\n",
        "lr = 0.01\n",
        "\n",
        "for e in [100,250,500,750,1000,2000]:\n",
        "    print(\"*\"*30)\n",
        "    print(\"PROCEDURE FOR EPOCH = \", e)\n",
        "    nn = NeuralNetwork(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE, learning_rate=lr)\n",
        "    nn.train(X=X_train, y=y_train_onehot, X_test=X_test, y_test=y_test_onehot, epochs=e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWL5dq7mCVyj",
        "outputId": "7b81ea64-2af8-4879-f31c-6d28966e58bb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************************\n",
            "PROCEDURE FOR EPOCH =  100\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "******************************\n",
            "PROCEDURE FOR EPOCH =  250\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "******************************\n",
            "PROCEDURE FOR EPOCH =  500\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "Epoch 250 - train_loss: 0.0945 , test_loss: 0.4284\n",
            "Epoch 260 - train_loss: 0.0932 , test_loss: 0.4237\n",
            "Epoch 270 - train_loss: 0.0918 , test_loss: 0.4184\n",
            "Epoch 280 - train_loss: 0.0904 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0889 , test_loss: 0.4062\n",
            "Epoch 300 - train_loss: 0.0873 , test_loss: 0.3994\n",
            "Epoch 310 - train_loss: 0.0856 , test_loss: 0.3924\n",
            "Epoch 320 - train_loss: 0.0838 , test_loss: 0.3853\n",
            "Epoch 330 - train_loss: 0.0819 , test_loss: 0.3784\n",
            "Epoch 340 - train_loss: 0.0799 , test_loss: 0.3717\n",
            "Epoch 350 - train_loss: 0.0779 , test_loss: 0.3653\n",
            "Epoch 360 - train_loss: 0.0758 , test_loss: 0.3594\n",
            "Epoch 370 - train_loss: 0.0737 , test_loss: 0.3541\n",
            "Epoch 380 - train_loss: 0.0716 , test_loss: 0.3494\n",
            "Epoch 390 - train_loss: 0.0696 , test_loss: 0.3453\n",
            "Epoch 400 - train_loss: 0.0675 , test_loss: 0.3418\n",
            "Epoch 410 - train_loss: 0.0655 , test_loss: 0.3388\n",
            "Epoch 420 - train_loss: 0.0636 , test_loss: 0.3365\n",
            "Epoch 430 - train_loss: 0.0616 , test_loss: 0.3346\n",
            "Epoch 440 - train_loss: 0.0597 , test_loss: 0.3332\n",
            "Epoch 450 - train_loss: 0.0579 , test_loss: 0.3322\n",
            "Epoch 460 - train_loss: 0.0560 , test_loss: 0.3315\n",
            "Epoch 470 - train_loss: 0.0542 , test_loss: 0.3310\n",
            "Epoch 480 - train_loss: 0.0525 , test_loss: 0.3308\n",
            "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
            "******************************\n",
            "PROCEDURE FOR EPOCH =  750\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "Epoch 250 - train_loss: 0.0945 , test_loss: 0.4284\n",
            "Epoch 260 - train_loss: 0.0932 , test_loss: 0.4237\n",
            "Epoch 270 - train_loss: 0.0918 , test_loss: 0.4184\n",
            "Epoch 280 - train_loss: 0.0904 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0889 , test_loss: 0.4062\n",
            "Epoch 300 - train_loss: 0.0873 , test_loss: 0.3994\n",
            "Epoch 310 - train_loss: 0.0856 , test_loss: 0.3924\n",
            "Epoch 320 - train_loss: 0.0838 , test_loss: 0.3853\n",
            "Epoch 330 - train_loss: 0.0819 , test_loss: 0.3784\n",
            "Epoch 340 - train_loss: 0.0799 , test_loss: 0.3717\n",
            "Epoch 350 - train_loss: 0.0779 , test_loss: 0.3653\n",
            "Epoch 360 - train_loss: 0.0758 , test_loss: 0.3594\n",
            "Epoch 370 - train_loss: 0.0737 , test_loss: 0.3541\n",
            "Epoch 380 - train_loss: 0.0716 , test_loss: 0.3494\n",
            "Epoch 390 - train_loss: 0.0696 , test_loss: 0.3453\n",
            "Epoch 400 - train_loss: 0.0675 , test_loss: 0.3418\n",
            "Epoch 410 - train_loss: 0.0655 , test_loss: 0.3388\n",
            "Epoch 420 - train_loss: 0.0636 , test_loss: 0.3365\n",
            "Epoch 430 - train_loss: 0.0616 , test_loss: 0.3346\n",
            "Epoch 440 - train_loss: 0.0597 , test_loss: 0.3332\n",
            "Epoch 450 - train_loss: 0.0579 , test_loss: 0.3322\n",
            "Epoch 460 - train_loss: 0.0560 , test_loss: 0.3315\n",
            "Epoch 470 - train_loss: 0.0542 , test_loss: 0.3310\n",
            "Epoch 480 - train_loss: 0.0525 , test_loss: 0.3308\n",
            "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
            "Epoch 500 - train_loss: 0.0491 , test_loss: 0.3308\n",
            "Epoch 510 - train_loss: 0.0474 , test_loss: 0.3312\n",
            "Epoch 520 - train_loss: 0.0458 , test_loss: 0.3317\n",
            "Epoch 530 - train_loss: 0.0443 , test_loss: 0.3325\n",
            "Epoch 540 - train_loss: 0.0428 , test_loss: 0.3336\n",
            "Epoch 550 - train_loss: 0.0413 , test_loss: 0.3351\n",
            "Epoch 560 - train_loss: 0.0399 , test_loss: 0.3370\n",
            "Epoch 570 - train_loss: 0.0386 , test_loss: 0.3393\n",
            "Epoch 580 - train_loss: 0.0373 , test_loss: 0.3419\n",
            "Epoch 590 - train_loss: 0.0361 , test_loss: 0.3449\n",
            "Epoch 600 - train_loss: 0.0349 , test_loss: 0.3481\n",
            "Epoch 610 - train_loss: 0.0338 , test_loss: 0.3515\n",
            "Epoch 620 - train_loss: 0.0328 , test_loss: 0.3550\n",
            "Epoch 630 - train_loss: 0.0318 , test_loss: 0.3586\n",
            "Epoch 640 - train_loss: 0.0309 , test_loss: 0.3620\n",
            "Epoch 650 - train_loss: 0.0300 , test_loss: 0.3654\n",
            "Epoch 660 - train_loss: 0.0291 , test_loss: 0.3686\n",
            "Epoch 670 - train_loss: 0.0284 , test_loss: 0.3716\n",
            "Epoch 680 - train_loss: 0.0276 , test_loss: 0.3745\n",
            "Epoch 690 - train_loss: 0.0269 , test_loss: 0.3772\n",
            "Epoch 700 - train_loss: 0.0263 , test_loss: 0.3796\n",
            "Epoch 710 - train_loss: 0.0256 , test_loss: 0.3819\n",
            "Epoch 720 - train_loss: 0.0250 , test_loss: 0.3840\n",
            "Epoch 730 - train_loss: 0.0245 , test_loss: 0.3859\n",
            "Epoch 740 - train_loss: 0.0240 , test_loss: 0.3877\n",
            "******************************\n",
            "PROCEDURE FOR EPOCH =  1000\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "Epoch 250 - train_loss: 0.0945 , test_loss: 0.4284\n",
            "Epoch 260 - train_loss: 0.0932 , test_loss: 0.4237\n",
            "Epoch 270 - train_loss: 0.0918 , test_loss: 0.4184\n",
            "Epoch 280 - train_loss: 0.0904 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0889 , test_loss: 0.4062\n",
            "Epoch 300 - train_loss: 0.0873 , test_loss: 0.3994\n",
            "Epoch 310 - train_loss: 0.0856 , test_loss: 0.3924\n",
            "Epoch 320 - train_loss: 0.0838 , test_loss: 0.3853\n",
            "Epoch 330 - train_loss: 0.0819 , test_loss: 0.3784\n",
            "Epoch 340 - train_loss: 0.0799 , test_loss: 0.3717\n",
            "Epoch 350 - train_loss: 0.0779 , test_loss: 0.3653\n",
            "Epoch 360 - train_loss: 0.0758 , test_loss: 0.3594\n",
            "Epoch 370 - train_loss: 0.0737 , test_loss: 0.3541\n",
            "Epoch 380 - train_loss: 0.0716 , test_loss: 0.3494\n",
            "Epoch 390 - train_loss: 0.0696 , test_loss: 0.3453\n",
            "Epoch 400 - train_loss: 0.0675 , test_loss: 0.3418\n",
            "Epoch 410 - train_loss: 0.0655 , test_loss: 0.3388\n",
            "Epoch 420 - train_loss: 0.0636 , test_loss: 0.3365\n",
            "Epoch 430 - train_loss: 0.0616 , test_loss: 0.3346\n",
            "Epoch 440 - train_loss: 0.0597 , test_loss: 0.3332\n",
            "Epoch 450 - train_loss: 0.0579 , test_loss: 0.3322\n",
            "Epoch 460 - train_loss: 0.0560 , test_loss: 0.3315\n",
            "Epoch 470 - train_loss: 0.0542 , test_loss: 0.3310\n",
            "Epoch 480 - train_loss: 0.0525 , test_loss: 0.3308\n",
            "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
            "Epoch 500 - train_loss: 0.0491 , test_loss: 0.3308\n",
            "Epoch 510 - train_loss: 0.0474 , test_loss: 0.3312\n",
            "Epoch 520 - train_loss: 0.0458 , test_loss: 0.3317\n",
            "Epoch 530 - train_loss: 0.0443 , test_loss: 0.3325\n",
            "Epoch 540 - train_loss: 0.0428 , test_loss: 0.3336\n",
            "Epoch 550 - train_loss: 0.0413 , test_loss: 0.3351\n",
            "Epoch 560 - train_loss: 0.0399 , test_loss: 0.3370\n",
            "Epoch 570 - train_loss: 0.0386 , test_loss: 0.3393\n",
            "Epoch 580 - train_loss: 0.0373 , test_loss: 0.3419\n",
            "Epoch 590 - train_loss: 0.0361 , test_loss: 0.3449\n",
            "Epoch 600 - train_loss: 0.0349 , test_loss: 0.3481\n",
            "Epoch 610 - train_loss: 0.0338 , test_loss: 0.3515\n",
            "Epoch 620 - train_loss: 0.0328 , test_loss: 0.3550\n",
            "Epoch 630 - train_loss: 0.0318 , test_loss: 0.3586\n",
            "Epoch 640 - train_loss: 0.0309 , test_loss: 0.3620\n",
            "Epoch 650 - train_loss: 0.0300 , test_loss: 0.3654\n",
            "Epoch 660 - train_loss: 0.0291 , test_loss: 0.3686\n",
            "Epoch 670 - train_loss: 0.0284 , test_loss: 0.3716\n",
            "Epoch 680 - train_loss: 0.0276 , test_loss: 0.3745\n",
            "Epoch 690 - train_loss: 0.0269 , test_loss: 0.3772\n",
            "Epoch 700 - train_loss: 0.0263 , test_loss: 0.3796\n",
            "Epoch 710 - train_loss: 0.0256 , test_loss: 0.3819\n",
            "Epoch 720 - train_loss: 0.0250 , test_loss: 0.3840\n",
            "Epoch 730 - train_loss: 0.0245 , test_loss: 0.3859\n",
            "Epoch 740 - train_loss: 0.0240 , test_loss: 0.3877\n",
            "Epoch 750 - train_loss: 0.0235 , test_loss: 0.3893\n",
            "Epoch 760 - train_loss: 0.0230 , test_loss: 0.3908\n",
            "Epoch 770 - train_loss: 0.0226 , test_loss: 0.3922\n",
            "Epoch 780 - train_loss: 0.0222 , test_loss: 0.3934\n",
            "Epoch 790 - train_loss: 0.0218 , test_loss: 0.3946\n",
            "Epoch 800 - train_loss: 0.0214 , test_loss: 0.3957\n",
            "Epoch 810 - train_loss: 0.0211 , test_loss: 0.3967\n",
            "Epoch 820 - train_loss: 0.0207 , test_loss: 0.3976\n",
            "Epoch 830 - train_loss: 0.0204 , test_loss: 0.3985\n",
            "Epoch 840 - train_loss: 0.0201 , test_loss: 0.3993\n",
            "Epoch 850 - train_loss: 0.0198 , test_loss: 0.4000\n",
            "Epoch 860 - train_loss: 0.0196 , test_loss: 0.4007\n",
            "Epoch 870 - train_loss: 0.0193 , test_loss: 0.4014\n",
            "Epoch 880 - train_loss: 0.0191 , test_loss: 0.4020\n",
            "Epoch 890 - train_loss: 0.0188 , test_loss: 0.4025\n",
            "Epoch 900 - train_loss: 0.0186 , test_loss: 0.4031\n",
            "Epoch 910 - train_loss: 0.0184 , test_loss: 0.4036\n",
            "Epoch 920 - train_loss: 0.0182 , test_loss: 0.4040\n",
            "Epoch 930 - train_loss: 0.0180 , test_loss: 0.4045\n",
            "Epoch 940 - train_loss: 0.0178 , test_loss: 0.4049\n",
            "Epoch 950 - train_loss: 0.0176 , test_loss: 0.4053\n",
            "Epoch 960 - train_loss: 0.0174 , test_loss: 0.4057\n",
            "Epoch 970 - train_loss: 0.0173 , test_loss: 0.4060\n",
            "Epoch 980 - train_loss: 0.0171 , test_loss: 0.4064\n",
            "Epoch 990 - train_loss: 0.0170 , test_loss: 0.4067\n",
            "******************************\n",
            "PROCEDURE FOR EPOCH =  2000\n",
            "Epoch 0 - train_loss: 0.2495 , test_loss: 0.4852\n",
            "Epoch 10 - train_loss: 0.2223 , test_loss: 0.4479\n",
            "Epoch 20 - train_loss: 0.2221 , test_loss: 0.4453\n",
            "Epoch 30 - train_loss: 0.2220 , test_loss: 0.4449\n",
            "Epoch 40 - train_loss: 0.2214 , test_loss: 0.4443\n",
            "Epoch 50 - train_loss: 0.2198 , test_loss: 0.4425\n",
            "Epoch 60 - train_loss: 0.2145 , test_loss: 0.4382\n",
            "Epoch 70 - train_loss: 0.2006 , test_loss: 0.4321\n",
            "Epoch 80 - train_loss: 0.1753 , test_loss: 0.4304\n",
            "Epoch 90 - train_loss: 0.1498 , test_loss: 0.4352\n",
            "Epoch 100 - train_loss: 0.1334 , test_loss: 0.4409\n",
            "Epoch 110 - train_loss: 0.1240 , test_loss: 0.4446\n",
            "Epoch 120 - train_loss: 0.1182 , test_loss: 0.4468\n",
            "Epoch 130 - train_loss: 0.1142 , test_loss: 0.4478\n",
            "Epoch 140 - train_loss: 0.1112 , test_loss: 0.4482\n",
            "Epoch 150 - train_loss: 0.1089 , test_loss: 0.4482\n",
            "Epoch 160 - train_loss: 0.1069 , test_loss: 0.4478\n",
            "Epoch 170 - train_loss: 0.1051 , test_loss: 0.4471\n",
            "Epoch 180 - train_loss: 0.1036 , test_loss: 0.4461\n",
            "Epoch 190 - train_loss: 0.1021 , test_loss: 0.4449\n",
            "Epoch 200 - train_loss: 0.1008 , test_loss: 0.4433\n",
            "Epoch 210 - train_loss: 0.0995 , test_loss: 0.4413\n",
            "Epoch 220 - train_loss: 0.0982 , test_loss: 0.4389\n",
            "Epoch 230 - train_loss: 0.0970 , test_loss: 0.4359\n",
            "Epoch 240 - train_loss: 0.0958 , test_loss: 0.4325\n",
            "Epoch 250 - train_loss: 0.0945 , test_loss: 0.4284\n",
            "Epoch 260 - train_loss: 0.0932 , test_loss: 0.4237\n",
            "Epoch 270 - train_loss: 0.0918 , test_loss: 0.4184\n",
            "Epoch 280 - train_loss: 0.0904 , test_loss: 0.4125\n",
            "Epoch 290 - train_loss: 0.0889 , test_loss: 0.4062\n",
            "Epoch 300 - train_loss: 0.0873 , test_loss: 0.3994\n",
            "Epoch 310 - train_loss: 0.0856 , test_loss: 0.3924\n",
            "Epoch 320 - train_loss: 0.0838 , test_loss: 0.3853\n",
            "Epoch 330 - train_loss: 0.0819 , test_loss: 0.3784\n",
            "Epoch 340 - train_loss: 0.0799 , test_loss: 0.3717\n",
            "Epoch 350 - train_loss: 0.0779 , test_loss: 0.3653\n",
            "Epoch 360 - train_loss: 0.0758 , test_loss: 0.3594\n",
            "Epoch 370 - train_loss: 0.0737 , test_loss: 0.3541\n",
            "Epoch 380 - train_loss: 0.0716 , test_loss: 0.3494\n",
            "Epoch 390 - train_loss: 0.0696 , test_loss: 0.3453\n",
            "Epoch 400 - train_loss: 0.0675 , test_loss: 0.3418\n",
            "Epoch 410 - train_loss: 0.0655 , test_loss: 0.3388\n",
            "Epoch 420 - train_loss: 0.0636 , test_loss: 0.3365\n",
            "Epoch 430 - train_loss: 0.0616 , test_loss: 0.3346\n",
            "Epoch 440 - train_loss: 0.0597 , test_loss: 0.3332\n",
            "Epoch 450 - train_loss: 0.0579 , test_loss: 0.3322\n",
            "Epoch 460 - train_loss: 0.0560 , test_loss: 0.3315\n",
            "Epoch 470 - train_loss: 0.0542 , test_loss: 0.3310\n",
            "Epoch 480 - train_loss: 0.0525 , test_loss: 0.3308\n",
            "Epoch 490 - train_loss: 0.0507 , test_loss: 0.3307\n",
            "Epoch 500 - train_loss: 0.0491 , test_loss: 0.3308\n",
            "Epoch 510 - train_loss: 0.0474 , test_loss: 0.3312\n",
            "Epoch 520 - train_loss: 0.0458 , test_loss: 0.3317\n",
            "Epoch 530 - train_loss: 0.0443 , test_loss: 0.3325\n",
            "Epoch 540 - train_loss: 0.0428 , test_loss: 0.3336\n",
            "Epoch 550 - train_loss: 0.0413 , test_loss: 0.3351\n",
            "Epoch 560 - train_loss: 0.0399 , test_loss: 0.3370\n",
            "Epoch 570 - train_loss: 0.0386 , test_loss: 0.3393\n",
            "Epoch 580 - train_loss: 0.0373 , test_loss: 0.3419\n",
            "Epoch 590 - train_loss: 0.0361 , test_loss: 0.3449\n",
            "Epoch 600 - train_loss: 0.0349 , test_loss: 0.3481\n",
            "Epoch 610 - train_loss: 0.0338 , test_loss: 0.3515\n",
            "Epoch 620 - train_loss: 0.0328 , test_loss: 0.3550\n",
            "Epoch 630 - train_loss: 0.0318 , test_loss: 0.3586\n",
            "Epoch 640 - train_loss: 0.0309 , test_loss: 0.3620\n",
            "Epoch 650 - train_loss: 0.0300 , test_loss: 0.3654\n",
            "Epoch 660 - train_loss: 0.0291 , test_loss: 0.3686\n",
            "Epoch 670 - train_loss: 0.0284 , test_loss: 0.3716\n",
            "Epoch 680 - train_loss: 0.0276 , test_loss: 0.3745\n",
            "Epoch 690 - train_loss: 0.0269 , test_loss: 0.3772\n",
            "Epoch 700 - train_loss: 0.0263 , test_loss: 0.3796\n",
            "Epoch 710 - train_loss: 0.0256 , test_loss: 0.3819\n",
            "Epoch 720 - train_loss: 0.0250 , test_loss: 0.3840\n",
            "Epoch 730 - train_loss: 0.0245 , test_loss: 0.3859\n",
            "Epoch 740 - train_loss: 0.0240 , test_loss: 0.3877\n",
            "Epoch 750 - train_loss: 0.0235 , test_loss: 0.3893\n",
            "Epoch 760 - train_loss: 0.0230 , test_loss: 0.3908\n",
            "Epoch 770 - train_loss: 0.0226 , test_loss: 0.3922\n",
            "Epoch 780 - train_loss: 0.0222 , test_loss: 0.3934\n",
            "Epoch 790 - train_loss: 0.0218 , test_loss: 0.3946\n",
            "Epoch 800 - train_loss: 0.0214 , test_loss: 0.3957\n",
            "Epoch 810 - train_loss: 0.0211 , test_loss: 0.3967\n",
            "Epoch 820 - train_loss: 0.0207 , test_loss: 0.3976\n",
            "Epoch 830 - train_loss: 0.0204 , test_loss: 0.3985\n",
            "Epoch 840 - train_loss: 0.0201 , test_loss: 0.3993\n",
            "Epoch 850 - train_loss: 0.0198 , test_loss: 0.4000\n",
            "Epoch 860 - train_loss: 0.0196 , test_loss: 0.4007\n",
            "Epoch 870 - train_loss: 0.0193 , test_loss: 0.4014\n",
            "Epoch 880 - train_loss: 0.0191 , test_loss: 0.4020\n",
            "Epoch 890 - train_loss: 0.0188 , test_loss: 0.4025\n",
            "Epoch 900 - train_loss: 0.0186 , test_loss: 0.4031\n",
            "Epoch 910 - train_loss: 0.0184 , test_loss: 0.4036\n",
            "Epoch 920 - train_loss: 0.0182 , test_loss: 0.4040\n",
            "Epoch 930 - train_loss: 0.0180 , test_loss: 0.4045\n",
            "Epoch 940 - train_loss: 0.0178 , test_loss: 0.4049\n",
            "Epoch 950 - train_loss: 0.0176 , test_loss: 0.4053\n",
            "Epoch 960 - train_loss: 0.0174 , test_loss: 0.4057\n",
            "Epoch 970 - train_loss: 0.0173 , test_loss: 0.4060\n",
            "Epoch 980 - train_loss: 0.0171 , test_loss: 0.4064\n",
            "Epoch 990 - train_loss: 0.0170 , test_loss: 0.4067\n",
            "Epoch 1000 - train_loss: 0.0168 , test_loss: 0.4070\n",
            "Epoch 1010 - train_loss: 0.0167 , test_loss: 0.4073\n",
            "Epoch 1020 - train_loss: 0.0166 , test_loss: 0.4075\n",
            "Epoch 1030 - train_loss: 0.0164 , test_loss: 0.4078\n",
            "Epoch 1040 - train_loss: 0.0163 , test_loss: 0.4081\n",
            "Epoch 1050 - train_loss: 0.0162 , test_loss: 0.4083\n",
            "Epoch 1060 - train_loss: 0.0161 , test_loss: 0.4085\n",
            "Epoch 1070 - train_loss: 0.0160 , test_loss: 0.4087\n",
            "Epoch 1080 - train_loss: 0.0158 , test_loss: 0.4089\n",
            "Epoch 1090 - train_loss: 0.0157 , test_loss: 0.4091\n",
            "Epoch 1100 - train_loss: 0.0156 , test_loss: 0.4093\n",
            "Epoch 1110 - train_loss: 0.0155 , test_loss: 0.4095\n",
            "Epoch 1120 - train_loss: 0.0154 , test_loss: 0.4097\n",
            "Epoch 1130 - train_loss: 0.0154 , test_loss: 0.4099\n",
            "Epoch 1140 - train_loss: 0.0153 , test_loss: 0.4100\n",
            "Epoch 1150 - train_loss: 0.0152 , test_loss: 0.4102\n",
            "Epoch 1160 - train_loss: 0.0151 , test_loss: 0.4103\n",
            "Epoch 1170 - train_loss: 0.0150 , test_loss: 0.4105\n",
            "Epoch 1180 - train_loss: 0.0149 , test_loss: 0.4106\n",
            "Epoch 1190 - train_loss: 0.0149 , test_loss: 0.4108\n",
            "Epoch 1200 - train_loss: 0.0148 , test_loss: 0.4109\n",
            "Epoch 1210 - train_loss: 0.0147 , test_loss: 0.4110\n",
            "Epoch 1220 - train_loss: 0.0146 , test_loss: 0.4111\n",
            "Epoch 1230 - train_loss: 0.0146 , test_loss: 0.4113\n",
            "Epoch 1240 - train_loss: 0.0145 , test_loss: 0.4114\n",
            "Epoch 1250 - train_loss: 0.0144 , test_loss: 0.4115\n",
            "Epoch 1260 - train_loss: 0.0144 , test_loss: 0.4116\n",
            "Epoch 1270 - train_loss: 0.0143 , test_loss: 0.4117\n",
            "Epoch 1280 - train_loss: 0.0143 , test_loss: 0.4118\n",
            "Epoch 1290 - train_loss: 0.0142 , test_loss: 0.4119\n",
            "Epoch 1300 - train_loss: 0.0141 , test_loss: 0.4120\n",
            "Epoch 1310 - train_loss: 0.0141 , test_loss: 0.4121\n",
            "Epoch 1320 - train_loss: 0.0140 , test_loss: 0.4122\n",
            "Epoch 1330 - train_loss: 0.0140 , test_loss: 0.4122\n",
            "Epoch 1340 - train_loss: 0.0139 , test_loss: 0.4123\n",
            "Epoch 1350 - train_loss: 0.0139 , test_loss: 0.4124\n",
            "Epoch 1360 - train_loss: 0.0138 , test_loss: 0.4125\n",
            "Epoch 1370 - train_loss: 0.0138 , test_loss: 0.4126\n",
            "Epoch 1380 - train_loss: 0.0137 , test_loss: 0.4126\n",
            "Epoch 1390 - train_loss: 0.0137 , test_loss: 0.4127\n",
            "Epoch 1400 - train_loss: 0.0136 , test_loss: 0.4128\n",
            "Epoch 1410 - train_loss: 0.0136 , test_loss: 0.4129\n",
            "Epoch 1420 - train_loss: 0.0135 , test_loss: 0.4129\n",
            "Epoch 1430 - train_loss: 0.0135 , test_loss: 0.4130\n",
            "Epoch 1440 - train_loss: 0.0135 , test_loss: 0.4131\n",
            "Epoch 1450 - train_loss: 0.0134 , test_loss: 0.4131\n",
            "Epoch 1460 - train_loss: 0.0134 , test_loss: 0.4132\n",
            "Epoch 1470 - train_loss: 0.0133 , test_loss: 0.4132\n",
            "Epoch 1480 - train_loss: 0.0133 , test_loss: 0.4133\n",
            "Epoch 1490 - train_loss: 0.0133 , test_loss: 0.4134\n",
            "Epoch 1500 - train_loss: 0.0132 , test_loss: 0.4134\n",
            "Epoch 1510 - train_loss: 0.0132 , test_loss: 0.4135\n",
            "Epoch 1520 - train_loss: 0.0132 , test_loss: 0.4135\n",
            "Epoch 1530 - train_loss: 0.0131 , test_loss: 0.4136\n",
            "Epoch 1540 - train_loss: 0.0131 , test_loss: 0.4136\n",
            "Epoch 1550 - train_loss: 0.0131 , test_loss: 0.4137\n",
            "Epoch 1560 - train_loss: 0.0130 , test_loss: 0.4137\n",
            "Epoch 1570 - train_loss: 0.0130 , test_loss: 0.4138\n",
            "Epoch 1580 - train_loss: 0.0130 , test_loss: 0.4138\n",
            "Epoch 1590 - train_loss: 0.0129 , test_loss: 0.4139\n",
            "Epoch 1600 - train_loss: 0.0129 , test_loss: 0.4139\n",
            "Epoch 1610 - train_loss: 0.0129 , test_loss: 0.4140\n",
            "Epoch 1620 - train_loss: 0.0128 , test_loss: 0.4140\n",
            "Epoch 1630 - train_loss: 0.0128 , test_loss: 0.4140\n",
            "Epoch 1640 - train_loss: 0.0128 , test_loss: 0.4141\n",
            "Epoch 1650 - train_loss: 0.0128 , test_loss: 0.4141\n",
            "Epoch 1660 - train_loss: 0.0127 , test_loss: 0.4142\n",
            "Epoch 1670 - train_loss: 0.0127 , test_loss: 0.4142\n",
            "Epoch 1680 - train_loss: 0.0127 , test_loss: 0.4142\n",
            "Epoch 1690 - train_loss: 0.0126 , test_loss: 0.4143\n",
            "Epoch 1700 - train_loss: 0.0126 , test_loss: 0.4143\n",
            "Epoch 1710 - train_loss: 0.0126 , test_loss: 0.4144\n",
            "Epoch 1720 - train_loss: 0.0126 , test_loss: 0.4144\n",
            "Epoch 1730 - train_loss: 0.0126 , test_loss: 0.4144\n",
            "Epoch 1740 - train_loss: 0.0125 , test_loss: 0.4145\n",
            "Epoch 1750 - train_loss: 0.0125 , test_loss: 0.4145\n",
            "Epoch 1760 - train_loss: 0.0125 , test_loss: 0.4145\n",
            "Epoch 1770 - train_loss: 0.0125 , test_loss: 0.4146\n",
            "Epoch 1780 - train_loss: 0.0124 , test_loss: 0.4146\n",
            "Epoch 1790 - train_loss: 0.0124 , test_loss: 0.4146\n",
            "Epoch 1800 - train_loss: 0.0124 , test_loss: 0.4147\n",
            "Epoch 1810 - train_loss: 0.0124 , test_loss: 0.4147\n",
            "Epoch 1820 - train_loss: 0.0123 , test_loss: 0.4147\n",
            "Epoch 1830 - train_loss: 0.0123 , test_loss: 0.4148\n",
            "Epoch 1840 - train_loss: 0.0123 , test_loss: 0.4148\n",
            "Epoch 1850 - train_loss: 0.0123 , test_loss: 0.4148\n",
            "Epoch 1860 - train_loss: 0.0123 , test_loss: 0.4148\n",
            "Epoch 1870 - train_loss: 0.0122 , test_loss: 0.4149\n",
            "Epoch 1880 - train_loss: 0.0122 , test_loss: 0.4149\n",
            "Epoch 1890 - train_loss: 0.0122 , test_loss: 0.4149\n",
            "Epoch 1900 - train_loss: 0.0122 , test_loss: 0.4150\n",
            "Epoch 1910 - train_loss: 0.0122 , test_loss: 0.4150\n",
            "Epoch 1920 - train_loss: 0.0122 , test_loss: 0.4150\n",
            "Epoch 1930 - train_loss: 0.0121 , test_loss: 0.4150\n",
            "Epoch 1940 - train_loss: 0.0121 , test_loss: 0.4151\n",
            "Epoch 1950 - train_loss: 0.0121 , test_loss: 0.4151\n",
            "Epoch 1960 - train_loss: 0.0121 , test_loss: 0.4151\n",
            "Epoch 1970 - train_loss: 0.0121 , test_loss: 0.4151\n",
            "Epoch 1980 - train_loss: 0.0120 , test_loss: 0.4152\n",
            "Epoch 1990 - train_loss: 0.0120 , test_loss: 0.4152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some guidelines:\n",
        "\n",
        "Breakdown of your ml model performance:\n",
        "\n",
        "Imagine building an image classifier(cat vs dog)\n",
        "\n",
        "Users upload images taken from their phones to your website and you output the label.\n",
        "\n",
        "Collect some data(ask some users and manually label) = expensive but accurate(true production data). Use this as your test set.\n",
        "\n",
        "Now we need training data = scrape the web and use heuristic rules say if the file name has  `cat.jpeg`, it's a cat.\n",
        "\n",
        "Gotcha: potential distribution mismatch between train and test.\n",
        "\n",
        "Step 1: \n",
        "Get a proxy for the best performance possible(there is always some irreducable error)\n",
        "\n",
        "- `Human level performance`\n",
        "\n",
        "Step 2\n",
        "\n",
        "Train set | Validation set | Test set | Production perfomance\n",
        "\n",
        "\n",
        "- Gap between train performance and human level performance = Bias in the model(proxy)\n",
        "\n",
        "- Gap between Train perf and validation perf = Variance in the model(proxy)\n",
        "\n",
        "- Gap between valid perf and test perf = dist mismatch.\n",
        "\n",
        "- Gap between deployment and test perf = overfitting on the dev set.(typically)\n",
        "\n",
        "\n",
        "\n",
        "Ultimate target = work well in deployment.\n",
        "\n",
        "\n",
        "1. what if human level perf is pretty low ?\n",
        "\n",
        "2. Human level perf is good but training error is high ?\n",
        "\n",
        "3. Gap between valid and train perf is high ?\n",
        "\n",
        "4. Gap between valid and test set is high ?\n",
        "\n",
        "5. What does it mean if we have train acc = 80% and valid = 85% ?\n",
        "\n",
        "6. Collecting data is expensive, can my errors indicate when to go about collecting more data ?"
      ],
      "metadata": {
        "id": "U0ji9jGvfQJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 ans: Bias (inability to represte the best possible model by our hypothesis class)\n",
        "\n",
        "- add features\n",
        "- use a complex model\n",
        "- reduce regularization\n",
        "\n",
        "3 ans:\n",
        "\n",
        "- small models\n",
        "- increase regularization\n",
        "- get more data [ you are doing well on train data but not generalizing well = not learning right pattern = collect more data and make it harder to fit thus possibly forcing to learn better patterns]\n",
        "\n",
        "4 ans:\n",
        "\n",
        "- use creativity.(upsampling etc)\n",
        "\n",
        "5 ans\n",
        "\n",
        "6.ans\n",
        "\n",
        "\n",
        "\n",
        "on different fractions of data = measure the train error and test error. If the gap remained same or converging = No use in more data\n",
        "\n",
        "else = collect more data (high -variance scenario)\n",
        "\n"
      ],
      "metadata": {
        "id": "sAolTTNujJks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes for grading**: NN initial answers depend on initialization. Award grades for sensible random intializations for W and zeros for b. All following answers are awarded only for succinct supporting statements.\n",
        "\n"
      ],
      "metadata": {
        "id": "ewUvb8PszuQP"
      }
    }
  ]
}